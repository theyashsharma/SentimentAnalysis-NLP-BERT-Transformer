{"cells":[{"cell_type":"markdown","metadata":{"id":"05KYS_pnO3Dq"},"source":["# Sentiment classification with English Twitter Datasets"]},{"cell_type":"markdown","metadata":{"id":"8_6lr4dbPZNZ"},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":728,"status":"ok","timestamp":1644064783203,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"O2ganP7WOcv-"},"outputs":[],"source":["import os\n","import re\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import csv\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"IpJdkXmSPKfj"},"source":["## 1 ) Loading Datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"E200bW4uv0hW","executionInfo":{"status":"ok","timestamp":1644064785618,"user_tz":-330,"elapsed":757,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"}}},"outputs":[],"source":["df1 = pd.read_csv('tweets_sqgames.csv')\n","df1 = df1.loc[:, ['text', 'sentiment']]\n","label_mapping = {\"Positive\": 1, \"Negative\":0}\n","df1 = df1[df1.sentiment != \"Neutral\"]\n","df1[\"sentiment\"] = df1[\"sentiment\"].map(label_mapping)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9zdyPvyDv0kq","executionInfo":{"status":"ok","timestamp":1644064785620,"user_tz":-330,"elapsed":6,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"}}},"outputs":[],"source":["df2 = pd.read_csv('Tweets.csv')\n","df2 = df2.loc[:, ['text', 'airline_sentiment']]\n","df2 = df2.rename(columns = {\"airline_sentiment\":\"sentiment\"})\n","label_mapping = {\"positive\": 1, \"negative\":0}\n","df2 = df2[df2.sentiment != \"neutral\"]\n","df2[\"sentiment\"] = df2[\"sentiment\"].map(label_mapping)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"wKAkAPNlv0r6","executionInfo":{"status":"ok","timestamp":1644064786303,"user_tz":-330,"elapsed":9,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"}}},"outputs":[],"source":["df3 = pd.read_csv('apple-twitter-sentiment-texts.csv')\n","label_mapping = {1: 1, -1:0}\n","df3 = df3[df3.sentiment != 0]\n","df3[\"sentiment\"] = df3[\"sentiment\"].map(label_mapping)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"f-d6W4F4v0vR","executionInfo":{"status":"ok","timestamp":1644064786303,"user_tz":-330,"elapsed":8,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"}}},"outputs":[],"source":["df4 = pd.read_csv('Apple-Twitter-Sentiment-DFE.csv', encoding=\"Latin-1\")\n","label_mapping = {\"5\": 1, \"1\":0}\n","df4 = df4[df4.sentiment != \"3\"]\n","df4 = df4[df4.sentiment != \"not_relevant\"]\n","df4[\"sentiment\"] = df4[\"sentiment\"].map(label_mapping)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"77x4gFaC6Dry","executionInfo":{"status":"ok","timestamp":1644064787267,"user_tz":-330,"elapsed":970,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"}}},"outputs":[],"source":["df5 = pd.read_csv('Reddit_Data.csv')\n","df5 = df5.rename(columns = {\"clean_comment\":\"text\", \"category\":\"sentiment\"})\n","label_mapping = {1: 1, -1:0}\n","df5 = df5[df5.sentiment != 0]\n","df5[\"sentiment\"] = df5[\"sentiment\"].map(label_mapping)"]},{"cell_type":"code","source":["frames = [df1, df2, df3, df4, df5]\n","merged_df = pd.concat(frames)"],"metadata":{"id":"ptc6kHzF5XBI","executionInfo":{"status":"ok","timestamp":1644064787810,"user_tz":-330,"elapsed":2,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1644064789590,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"M0LtDH9_O1pm"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X = merged_df.text.values\n","y = merged_df.sentiment.values"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1644064790271,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"5H2DVOPHO1sE"},"outputs":[],"source":["X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n","X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"]},{"cell_type":"markdown","metadata":{"id":"HakjkIbYY-EZ"},"source":["## 3 ) Deep Learning Approach"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5786,"status":"ok","timestamp":1644064796676,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"-oBZeFVSO2AH","outputId":"21313b77-439e-44ef-9ec9-b5106d9d871b"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","Device name: Tesla K80\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9298,"status":"ok","timestamp":1644064805970,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"AI790cijO2GE","outputId":"bdc09ea3-d42f-4394-8e42-b68dca3685da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 12.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 48.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 43.8 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 39.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1644064805971,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"SLMSbnC5O2DJ"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6418,"status":"ok","timestamp":1644064812386,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"xMmgFGdOO2Jh","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["ec5b15d2ff0d467ca5a3a7268d57b2c5","b96edf8d738b4c2c8c3c8978d20eb6cf","a1e4807f84b144d88743feca09743f77","fe1d86ba7dd24c66a0e0a8d0a3ec2db1","78f88b5131bd4bf7be4dcbcfec222002","21ffc32c92fe40ca9f18b96ab1155574","f9724a8d23694fd09a1c5f1bf964030b","1576e739cab8417f9381d20ad361ffcb","2ed9d674c47745c5b2ed1d91890a6ad5","1d19c6c0928247cca9f79b91802c6f15","1d5209c5a08b4bc5b5e9009307989b34","42d580795a824ff8ae1c9334bccdd237","cdcc06d014fe418fb525b352bb96be2d","94ddfdfa6d2f4c26a5aa28b64b93c611","1d6f082a6639424a9db07f1ddf573455","fe04f46822b74a3dbd58c9df82c9459a","8cf662a2ed634f0098461aeb3f5c3a08","14151b067a11486597cad988588f16ab","958e2f2127754713bba95b7d6f283d74","d934026808154ad89533166d457b2cc0","722008a704ac4e919c2a7c3e6bce43a4","a29fb0ade805460bab3c4e7a0b82f283","067455f6b0884594bddb36e8b3436707","678910d0099a4f408c4b0ab7e4baedfc","1de91868528e49d78034f50fb9147ff7","39d5d7a7c8cc4e73bb8dfeb543f15aa0","b65590cd324a4f91a3731138a79ca6da","41d298e0ad96495f8b5433211aa460ef","3c6bfa329e3d415cb619dad3baeb1eb1","9b05e2ebd542400bb5d024840ed3f272","cf4ada94f68c4a8ebbdbda42b075fde4","d3554d7052604eafa737ab0f95a16da9","33a99a40fb8c4e9aab39cad410a67037","4664291be2084e0183452ff9be397124","c60008adb8e548b4a284cca970bc796e","16aa356699db43babb6fb3aac6720e0c","9e5f0121dbe242daae06bb0068be6b74","cdaa95d84a074d42a12c9e9ad2b82c03","e13cc76ef0a44b4ea07801597a029b42","7480697222fd4282b454dff91fb9bcae","fe649716219840bc89e3c099f438a397","ad02c8340e7f4a5091a177400cb56faf","218736a0fe8f46bbac29a563722cab32","895ebb4131644d7b90682687e1d2d8b4"]},"outputId":"a91b9a7b-9742-45b1-a5cc-67b352d4b37e"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec5b15d2ff0d467ca5a3a7268d57b2c5","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/291 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42d580795a824ff8ae1c9334bccdd237","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/768 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"067455f6b0884594bddb36e8b3436707","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4664291be2084e0183452ff9be397124","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{}}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\")"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1644064812387,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"o2TenWCGZw7I"},"outputs":[],"source":["def text_preprocessing(text):\n","    \"\"\"\n","    - Remove entity mentions (eg. '@united')\n","    - Correct errors (eg. '&amp;' to '&')\n","    @param    text (str): a string to be processed.\n","    @return   text (Str): the processed string.\n","    \"\"\"\n","\n","    # Normalize unicode encoding\n","    text = unicodedata.normalize('NFC', text)\n","    # Remove '@name'\n","    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n","\n","    # Replace '&amp;' with '&'\n","    text = re.sub(r'&amp;', '&', text)\n","\n","    # Remove trailing whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    #Remove URLs\n","    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '<URL>', text)\n","\n","\n","    return text"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1644064812387,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"Lp6sDjV9L2hQ"},"outputs":[],"source":["def remove_emojis(sent):\n","    text =  emoji.demojize(sent)\n","    text= re.sub(r'(:[!_\\-\\w]+:)', '', text)\n","    return text\n","    \n","def text_preprocessing_no_emojis(text):\n","    \"\"\"\n","    - Remove entity mentions (eg. '@united')\n","    - Correct errors (eg. '&amp;' to '&')\n","    @param    text (str): a string to be processed.\n","    @return   text (Str): the processed string.\n","    \"\"\"\n","  \n","    # Remove emojis\n","    text = remove_emojis(text)\n","\n","    return text_preprocessing(text)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4507,"status":"ok","timestamp":1644064816890,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"gklJcuwha7hu","outputId":"726a3ed1-ca13-4135-8c83-3727ad9b4a8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-1.6.3.tar.gz (174 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 71 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 102 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 112 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 122 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 133 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 143 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 153 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 12.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=3900b85e3660888985b42254708d7d77a522e89689a4df60d6e42740d16bce9c\n","  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.6.3\n"]}],"source":["!pip install emoji"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1644064816890,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"8fjl5AftaQU7"},"outputs":[],"source":["import emoji\n","import unicodedata\n","def preprocessing_for_bert(data, version=\"mini\", text_preprocessing_fn = text_preprocessing_no_emojis):\n","    \"\"\"Perform required preprocessing steps for pretrained BERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    # Create empty lists to store outputs\n","    input_ids = []\n","    attention_masks = []\n","    tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\")# if version == \"mini\" else AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n","\n","    # For every sentence...\n","    for i,sent in enumerate(data):\n","        # `encode_plus` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Create attention mask\n","        #    (6) Return a dictionary of outputs\n","        encoded_sent = tokenizer.encode_plus(\n","            text=text_preprocessing_fn(sent),  # Preprocess sentence\n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                  # Max length to truncate/pad\n","            padding='max_length',        # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True,     # Return attention mask\n","            truncation = True \n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39400,"status":"ok","timestamp":1644064856270,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"PCm99bITa5hL","outputId":"a131fd75-1e80-4937-99dd-a43a34f84231"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  When life hits and the same time poverty strikes you\n","Gong Yoo : Lets play a game \n","#SquidGame #Netflix https://t.co/Cx7ifmZ8cN\n","Token IDs:  [101, 2043, 2166, 4978, 1998, 1996, 2168, 2051, 5635, 9326, 2017, 17242, 26823, 1024, 11082, 2377, 1037, 2208, 1001, 26852, 16650, 1001, 20907, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1039, 2595, 2581, 10128, 2213, 2480, 2620, 2278, 2078, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Tokenizing data...\n"]}],"source":["# Specify `MAX_LEN`\n","MAX_LEN =  280\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)\n","val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1644064856271,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"U7iTT16LbLni"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1644064856271,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"32gvSQ9pb9Fr","outputId":"91dcc2d6-263c-46ad-c891-bed8c958d107"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 26.6 ms, sys: 1.94 ms, total: 28.6 ms\n","Wall time: 30.3 ms\n"]}],"source":["%%time\n","import torch\n","import torch.nn as nn\n","from transformers import BertModel\n","\n","# Create the BertClassfier class\n","class BertClassifier(nn.Module):\n","    \"\"\"Bert Model for Classification Tasks.\n","    \"\"\"\n","    def __init__(self, freeze_bert=False, version=\"mini\"):\n","        \"\"\"\n","        @param    bert: a BertModel object\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super(BertClassifier, self).__init__()\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in = 768\n","        H, D_out = 50, 2\n","\n","        # Instantiate BERT model\n","        self.bert = AutoModel.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\")\n","        self.lstm = nn.LSTM(768, 768, bidirectional=True)\n","        # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Feed input to BERT and the classifier to compute logits.\n","        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n","                      max_length)\n","        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n","                      information with shape (batch_size, max_length)\n","        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n","                      num_labels)\n","        \"\"\"\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2133,"status":"ok","timestamp":1644064858388,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"z7RwU_UBPHUh"},"outputs":[],"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","from torch.optim import SparseAdam, Adam\n","def initialize_model(epochs=4, version=\"base\"):\n","    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n","    \"\"\"\n","    # Instantiate Bert Classifier\n","    bert_classifier = BertClassifier(freeze_bert=False, version=version)\n","    # Tell PyTorch to run the model on GPU\n","    bert_classifier.to(device)\n","\n","    # Create the optimizer\n","    optimizer = torch.optim.AdamW(params=list(bert_classifier.parameters()),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Set up the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1644064858390,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04578811976861982751"},"user_tz":-330},"id":"DwlxCFVUPHXN"},"outputs":[],"source":["import random\n","import time\n","import torch\n","import torch.nn as nn\n","\n","# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n","    \"\"\"Train the BertClassifier model.\n","    \"\"\"\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","        # Print the header of the result table\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and the learning rate\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After the completion of each training epoch, measure the model's performance\n","            # on our validation set.\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            \n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    \n","    print(\"Training complete!\")\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","       \n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9fa3f408c31d4fb5b21267abab850da7","8291c9a9205c4aa3b64b354537969f7e","d2610201c58040c38d2e85aaa707bab6","ff8f883b21ed4347bbcb387bc1defbae","8e90d48f22f84e7ea6e3d3f735dd1e56","975856f440614a07ae3399209eeb4391","a03b37e8795949b4864f00f835bcb165","aa945d6dcd004790962e6ed8f74c0fed","0190563d26304024b8516de3c2df6aee","a45ee8984c9241c29f80227616740329","eff5d73f01ad44b98e552d165c9819a8"]},"id":"I0K_YbfyO1TR","outputId":"c2c6a77f-f88b-403e-c819-720a83e15bad"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fa3f408c31d4fb5b21267abab850da7","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bhadresh-savani/distilbert-base-uncased-emotion were not used when initializing DistilBertModel: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   1    |   20    |   0.586751   |     -      |     -     |   16.40  \n","   1    |   40    |   0.594181   |     -      |     -     |   15.65  \n","   1    |   60    |   0.512800   |     -      |     -     |   15.69  \n","   1    |   80    |   0.449209   |     -      |     -     |   15.64  \n","   1    |   100   |   0.449488   |     -      |     -     |   15.59  \n","   1    |   120   |   0.466566   |     -      |     -     |   15.59  \n","   1    |   140   |   0.476887   |     -      |     -     |   15.58  \n","   1    |   160   |   0.450233   |     -      |     -     |   15.58  \n","   1    |   180   |   0.355887   |     -      |     -     |   15.57  \n","   1    |   200   |   0.402805   |     -      |     -     |   15.59  \n","   1    |   220   |   0.395715   |     -      |     -     |   15.57  \n","   1    |   240   |   0.375269   |     -      |     -     |   15.59  \n","   1    |   260   |   0.400760   |     -      |     -     |   15.58  \n","   1    |   280   |   0.314443   |     -      |     -     |   15.55  \n","   1    |   300   |   0.325611   |     -      |     -     |   15.56  \n","   1    |   320   |   0.313223   |     -      |     -     |   15.56  \n","   1    |   340   |   0.338463   |     -      |     -     |   15.55  \n","   1    |   360   |   0.317765   |     -      |     -     |   15.53  \n","   1    |   380   |   0.288916   |     -      |     -     |   15.53  \n","   1    |   400   |   0.313702   |     -      |     -     |   15.52  \n","   1    |   420   |   0.313309   |     -      |     -     |   15.53  \n","   1    |   440   |   0.375165   |     -      |     -     |   15.54  \n","   1    |   460   |   0.329968   |     -      |     -     |   15.55  \n","   1    |   480   |   0.269708   |     -      |     -     |   15.57  \n","   1    |   500   |   0.286570   |     -      |     -     |   15.54  \n","   1    |   520   |   0.405482   |     -      |     -     |   15.57  \n","   1    |   540   |   0.318847   |     -      |     -     |   15.55  \n","   1    |   560   |   0.232969   |     -      |     -     |   15.56  \n","   1    |   580   |   0.271297   |     -      |     -     |   15.55  \n","   1    |   600   |   0.300049   |     -      |     -     |   15.57  \n","   1    |   620   |   0.289069   |     -      |     -     |   15.58  \n","   1    |   640   |   0.318074   |     -      |     -     |   15.57  \n","   1    |   660   |   0.274889   |     -      |     -     |   15.55  \n","   1    |   680   |   0.297383   |     -      |     -     |   15.56  \n","   1    |   700   |   0.272362   |     -      |     -     |   15.54  \n","   1    |   720   |   0.244095   |     -      |     -     |   15.56  \n","   1    |   740   |   0.337823   |     -      |     -     |   15.55  \n","   1    |   760   |   0.216564   |     -      |     -     |   15.59  \n","   1    |   780   |   0.207843   |     -      |     -     |   15.54  \n","   1    |   800   |   0.303547   |     -      |     -     |   15.58  \n","   1    |   820   |   0.218993   |     -      |     -     |   15.60  \n","   1    |   840   |   0.338313   |     -      |     -     |   15.58  \n","   1    |   860   |   0.302908   |     -      |     -     |   15.54  \n","   1    |   880   |   0.235408   |     -      |     -     |   15.54  \n","   1    |   900   |   0.310198   |     -      |     -     |   15.55  \n","   1    |   920   |   0.241304   |     -      |     -     |   15.55  \n","   1    |   940   |   0.361095   |     -      |     -     |   15.54  \n","   1    |   960   |   0.233414   |     -      |     -     |   15.54  \n","   1    |   980   |   0.334792   |     -      |     -     |   15.53  \n","   1    |  1000   |   0.282745   |     -      |     -     |   15.54  \n","   1    |  1020   |   0.230171   |     -      |     -     |   15.48  \n","   1    |  1040   |   0.253942   |     -      |     -     |   15.53  \n","   1    |  1060   |   0.206006   |     -      |     -     |   15.55  \n","   1    |  1080   |   0.330481   |     -      |     -     |   15.56  \n","   1    |  1100   |   0.260016   |     -      |     -     |   15.53  \n","   1    |  1120   |   0.204931   |     -      |     -     |   15.54  \n","   1    |  1140   |   0.353770   |     -      |     -     |   15.51  \n","   1    |  1160   |   0.303015   |     -      |     -     |   15.51  \n","   1    |  1180   |   0.245171   |     -      |     -     |   15.53  \n","   1    |  1200   |   0.254696   |     -      |     -     |   15.55  \n","   1    |  1220   |   0.207294   |     -      |     -     |   15.55  \n","   1    |  1240   |   0.246867   |     -      |     -     |   15.54  \n","   1    |  1260   |   0.230565   |     -      |     -     |   15.53  \n","   1    |  1280   |   0.211142   |     -      |     -     |   15.52  \n","   1    |  1300   |   0.264241   |     -      |     -     |   15.53  \n","   1    |  1320   |   0.242231   |     -      |     -     |   15.53  \n","   1    |  1340   |   0.227396   |     -      |     -     |   15.53  \n","   1    |  1360   |   0.227762   |     -      |     -     |   15.55  \n","   1    |  1380   |   0.278292   |     -      |     -     |   15.54  \n","   1    |  1400   |   0.249461   |     -      |     -     |   15.53  \n","   1    |  1420   |   0.210274   |     -      |     -     |   15.56  \n","   1    |  1440   |   0.183346   |     -      |     -     |   15.55  \n","   1    |  1460   |   0.212781   |     -      |     -     |   15.54  \n","   1    |  1480   |   0.277531   |     -      |     -     |   15.54  \n","   1    |  1500   |   0.265501   |     -      |     -     |   15.55  \n","   1    |  1520   |   0.155047   |     -      |     -     |   15.52  \n","   1    |  1540   |   0.208681   |     -      |     -     |   15.51  \n","   1    |  1560   |   0.304130   |     -      |     -     |   15.52  \n","   1    |  1580   |   0.150814   |     -      |     -     |   15.53  \n","   1    |  1600   |   0.257940   |     -      |     -     |   15.53  \n","   1    |  1620   |   0.209946   |     -      |     -     |   15.55  \n","   1    |  1640   |   0.255825   |     -      |     -     |   15.54  \n","   1    |  1660   |   0.214418   |     -      |     -     |   15.53  \n","   1    |  1680   |   0.288072   |     -      |     -     |   15.53  \n","   1    |  1700   |   0.168485   |     -      |     -     |   15.50  \n","   1    |  1720   |   0.227290   |     -      |     -     |   15.53  \n","   1    |  1740   |   0.249326   |     -      |     -     |   15.53  \n","   1    |  1760   |   0.208523   |     -      |     -     |   15.53  \n","   1    |  1780   |   0.178068   |     -      |     -     |   15.56  \n","   1    |  1800   |   0.233933   |     -      |     -     |   15.54  \n","   1    |  1820   |   0.228150   |     -      |     -     |   15.56  \n","   1    |  1840   |   0.232661   |     -      |     -     |   15.54  \n","   1    |  1860   |   0.163896   |     -      |     -     |   15.52  \n","   1    |  1880   |   0.268427   |     -      |     -     |   15.55  \n","   1    |  1900   |   0.265083   |     -      |     -     |   15.54  \n","   1    |  1920   |   0.222698   |     -      |     -     |   15.56  \n","   1    |  1940   |   0.169492   |     -      |     -     |   15.52  \n","   1    |  1960   |   0.190754   |     -      |     -     |   15.54  \n","   1    |  1980   |   0.185535   |     -      |     -     |   15.53  \n","   1    |  2000   |   0.218610   |     -      |     -     |   15.56  \n","   1    |  2020   |   0.321617   |     -      |     -     |   15.55  \n","   1    |  2040   |   0.194382   |     -      |     -     |   15.56  \n","   1    |  2060   |   0.200443   |     -      |     -     |   15.57  \n","   1    |  2080   |   0.150016   |     -      |     -     |   15.54  \n","   1    |  2100   |   0.216801   |     -      |     -     |   15.57  \n","   1    |  2120   |   0.198496   |     -      |     -     |   15.58  \n","   1    |  2140   |   0.176295   |     -      |     -     |   15.55  \n","   1    |  2160   |   0.186574   |     -      |     -     |   15.57  \n","   1    |  2180   |   0.195626   |     -      |     -     |   15.53  \n","   1    |  2200   |   0.224789   |     -      |     -     |   15.54  \n","   1    |  2220   |   0.206632   |     -      |     -     |   15.54  \n","   1    |  2240   |   0.173307   |     -      |     -     |   15.56  \n","   1    |  2260   |   0.136107   |     -      |     -     |   15.56  \n","   1    |  2280   |   0.186843   |     -      |     -     |   15.54  \n","   1    |  2300   |   0.253212   |     -      |     -     |   15.53  \n","   1    |  2320   |   0.231864   |     -      |     -     |   15.53  \n","   1    |  2340   |   0.178197   |     -      |     -     |   15.54  \n","   1    |  2360   |   0.323438   |     -      |     -     |   15.52  \n","   1    |  2380   |   0.262871   |     -      |     -     |   15.53  \n","   1    |  2400   |   0.153422   |     -      |     -     |   15.56  \n","   1    |  2420   |   0.218996   |     -      |     -     |   15.58  \n","   1    |  2440   |   0.201333   |     -      |     -     |   15.54  \n","   1    |  2460   |   0.217666   |     -      |     -     |   15.54  \n","   1    |  2480   |   0.220480   |     -      |     -     |   15.56  \n","   1    |  2500   |   0.174824   |     -      |     -     |   15.54  \n","   1    |  2520   |   0.174870   |     -      |     -     |   15.56  \n","   1    |  2540   |   0.237132   |     -      |     -     |   15.56  \n","   1    |  2560   |   0.164839   |     -      |     -     |   15.55  \n","   1    |  2580   |   0.196324   |     -      |     -     |   15.55  \n","   1    |  2600   |   0.212172   |     -      |     -     |   15.55  \n","   1    |  2620   |   0.188311   |     -      |     -     |   15.53  \n","   1    |  2640   |   0.185997   |     -      |     -     |   15.55  \n","   1    |  2660   |   0.255218   |     -      |     -     |   15.56  \n","   1    |  2680   |   0.233282   |     -      |     -     |   15.57  \n","   1    |  2700   |   0.156766   |     -      |     -     |   15.56  \n","   1    |  2720   |   0.189635   |     -      |     -     |   15.57  \n","   1    |  2740   |   0.236795   |     -      |     -     |   15.58  \n","   1    |  2760   |   0.168034   |     -      |     -     |   15.55  \n","   1    |  2780   |   0.220050   |     -      |     -     |   15.54  \n","   1    |  2800   |   0.306415   |     -      |     -     |   15.56  \n","   1    |  2820   |   0.245628   |     -      |     -     |   15.56  \n","   1    |  2840   |   0.176252   |     -      |     -     |   15.52  \n","   1    |  2860   |   0.179203   |     -      |     -     |   15.55  \n","   1    |  2880   |   0.193186   |     -      |     -     |   15.54  \n","   1    |  2900   |   0.180005   |     -      |     -     |   15.57  \n","   1    |  2920   |   0.167048   |     -      |     -     |   15.54  \n","   1    |  2940   |   0.252707   |     -      |     -     |   15.56  \n","   1    |  2960   |   0.124756   |     -      |     -     |   15.53  \n","   1    |  2980   |   0.161178   |     -      |     -     |   15.56  \n","   1    |  3000   |   0.182455   |     -      |     -     |   15.57  \n","   1    |  3020   |   0.267893   |     -      |     -     |   15.55  \n","   1    |  3040   |   0.156280   |     -      |     -     |   15.56  \n","   1    |  3060   |   0.091749   |     -      |     -     |   15.57  \n","   1    |  3080   |   0.171741   |     -      |     -     |   15.55  \n","   1    |  3100   |   0.236263   |     -      |     -     |   15.56  \n","   1    |  3120   |   0.140692   |     -      |     -     |   15.56  \n","   1    |  3140   |   0.195338   |     -      |     -     |   15.54  \n","   1    |  3160   |   0.196120   |     -      |     -     |   15.56  \n","   1    |  3180   |   0.134026   |     -      |     -     |   15.57  \n","   1    |  3200   |   0.194155   |     -      |     -     |   15.55  \n","   1    |  3220   |   0.193405   |     -      |     -     |   15.56  \n","   1    |  3240   |   0.208635   |     -      |     -     |   15.54  \n","   1    |  3260   |   0.095906   |     -      |     -     |   15.54  \n","   1    |  3280   |   0.210667   |     -      |     -     |   15.56  \n","   1    |  3300   |   0.184663   |     -      |     -     |   15.56  \n","   1    |  3320   |   0.122677   |     -      |     -     |   15.54  \n","   1    |  3340   |   0.212484   |     -      |     -     |   15.56  \n","   1    |  3360   |   0.213441   |     -      |     -     |   15.56  \n","   1    |  3380   |   0.225813   |     -      |     -     |   15.57  \n","   1    |  3400   |   0.148771   |     -      |     -     |   15.56  \n","   1    |  3420   |   0.165331   |     -      |     -     |   15.56  \n","   1    |  3440   |   0.136659   |     -      |     -     |   15.54  \n","   1    |  3460   |   0.175669   |     -      |     -     |   15.54  \n","   1    |  3480   |   0.181392   |     -      |     -     |   15.56  \n","   1    |  3500   |   0.170542   |     -      |     -     |   15.56  \n","   1    |  3520   |   0.150399   |     -      |     -     |   15.54  \n","   1    |  3540   |   0.157894   |     -      |     -     |   15.55  \n","   1    |  3560   |   0.255481   |     -      |     -     |   15.55  \n","   1    |  3580   |   0.167607   |     -      |     -     |   15.54  \n","   1    |  3600   |   0.158444   |     -      |     -     |   15.55  \n","   1    |  3620   |   0.211442   |     -      |     -     |   15.57  \n","   1    |  3640   |   0.227081   |     -      |     -     |   15.54  \n","   1    |  3660   |   0.205814   |     -      |     -     |   15.60  \n","   1    |  3680   |   0.154891   |     -      |     -     |   15.57  \n","   1    |  3700   |   0.143992   |     -      |     -     |   15.57  \n","   1    |  3720   |   0.202641   |     -      |     -     |   15.57  \n","   1    |  3740   |   0.196559   |     -      |     -     |   15.56  \n","   1    |  3760   |   0.166023   |     -      |     -     |   15.55  \n","   1    |  3780   |   0.193652   |     -      |     -     |   15.56  \n","   1    |  3800   |   0.193799   |     -      |     -     |   15.56  \n","   1    |  3820   |   0.177207   |     -      |     -     |   15.55  \n","   1    |  3840   |   0.230071   |     -      |     -     |   15.52  \n","   1    |  3860   |   0.153266   |     -      |     -     |   15.56  \n","   1    |  3880   |   0.139889   |     -      |     -     |   15.57  \n","   1    |  3900   |   0.164226   |     -      |     -     |   15.57  \n","   1    |  3920   |   0.216900   |     -      |     -     |   15.55  \n","   1    |  3940   |   0.166038   |     -      |     -     |   15.54  \n","   1    |  3950   |   0.108551   |     -      |     -     |   7.78   \n","----------------------------------------------------------------------\n","   1    |    -    |   0.239055   |  0.169097  |   94.20   |  3196.65 \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |   20    |   0.084939   |     -      |     -     |   16.34  \n","   2    |   40    |   0.120379   |     -      |     -     |   15.53  \n","   2    |   60    |   0.104466   |     -      |     -     |   15.57  \n","   2    |   80    |   0.108829   |     -      |     -     |   15.55  \n","   2    |   100   |   0.161603   |     -      |     -     |   15.58  \n","   2    |   120   |   0.085514   |     -      |     -     |   15.52  \n","   2    |   140   |   0.112957   |     -      |     -     |   15.55  \n","   2    |   160   |   0.109628   |     -      |     -     |   15.57  \n","   2    |   180   |   0.086068   |     -      |     -     |   15.53  \n","   2    |   200   |   0.119882   |     -      |     -     |   15.56  \n","   2    |   220   |   0.114345   |     -      |     -     |   15.51  \n","   2    |   240   |   0.103682   |     -      |     -     |   15.56  \n","   2    |   260   |   0.139469   |     -      |     -     |   15.57  \n","   2    |   280   |   0.136456   |     -      |     -     |   15.56  \n","   2    |   300   |   0.060804   |     -      |     -     |   15.54  \n","   2    |   320   |   0.105573   |     -      |     -     |   15.55  \n","   2    |   340   |   0.177299   |     -      |     -     |   15.57  \n","   2    |   360   |   0.146795   |     -      |     -     |   15.57  \n","   2    |   380   |   0.107441   |     -      |     -     |   15.56  \n","   2    |   400   |   0.154288   |     -      |     -     |   15.56  \n","   2    |   420   |   0.116109   |     -      |     -     |   15.56  \n","   2    |   440   |   0.107539   |     -      |     -     |   15.55  \n","   2    |   460   |   0.148520   |     -      |     -     |   15.54  \n","   2    |   480   |   0.205242   |     -      |     -     |   15.56  \n","   2    |   500   |   0.119547   |     -      |     -     |   15.55  \n","   2    |   520   |   0.110408   |     -      |     -     |   15.55  \n","   2    |   540   |   0.103045   |     -      |     -     |   15.56  \n","   2    |   560   |   0.165560   |     -      |     -     |   15.54  \n","   2    |   580   |   0.076517   |     -      |     -     |   15.57  \n","   2    |   600   |   0.125749   |     -      |     -     |   15.56  \n","   2    |   620   |   0.060059   |     -      |     -     |   15.56  \n","   2    |   640   |   0.148260   |     -      |     -     |   15.53  \n","   2    |   660   |   0.066974   |     -      |     -     |   15.53  \n","   2    |   680   |   0.137404   |     -      |     -     |   15.56  \n","   2    |   700   |   0.135003   |     -      |     -     |   15.59  \n","   2    |   720   |   0.114996   |     -      |     -     |   15.54  \n","   2    |   740   |   0.066070   |     -      |     -     |   15.55  \n","   2    |   760   |   0.119908   |     -      |     -     |   15.54  \n","   2    |   780   |   0.087780   |     -      |     -     |   15.55  \n","   2    |   800   |   0.139333   |     -      |     -     |   15.55  \n","   2    |   820   |   0.131376   |     -      |     -     |   15.59  \n","   2    |   840   |   0.141387   |     -      |     -     |   15.56  \n","   2    |   860   |   0.123521   |     -      |     -     |   15.55  \n","   2    |   880   |   0.087094   |     -      |     -     |   15.57  \n","   2    |   900   |   0.160906   |     -      |     -     |   15.56  \n","   2    |   920   |   0.124330   |     -      |     -     |   15.57  \n","   2    |   940   |   0.087539   |     -      |     -     |   15.57  \n","   2    |   960   |   0.162722   |     -      |     -     |   15.54  \n","   2    |   980   |   0.082927   |     -      |     -     |   15.53  \n","   2    |  1000   |   0.155383   |     -      |     -     |   15.54  \n","   2    |  1020   |   0.166724   |     -      |     -     |   15.56  \n","   2    |  1040   |   0.083093   |     -      |     -     |   15.55  \n","   2    |  1060   |   0.116610   |     -      |     -     |   15.53  \n","   2    |  1080   |   0.118205   |     -      |     -     |   15.57  \n","   2    |  1100   |   0.074166   |     -      |     -     |   15.55  \n","   2    |  1120   |   0.171047   |     -      |     -     |   15.59  \n","   2    |  1140   |   0.121568   |     -      |     -     |   15.55  \n","   2    |  1160   |   0.068775   |     -      |     -     |   15.56  \n","   2    |  1180   |   0.131353   |     -      |     -     |   15.59  \n","   2    |  1200   |   0.115756   |     -      |     -     |   15.56  \n","   2    |  1220   |   0.126553   |     -      |     -     |   15.59  \n","   2    |  1240   |   0.083354   |     -      |     -     |   15.56  \n","   2    |  1260   |   0.103418   |     -      |     -     |   15.56  \n","   2    |  1280   |   0.137375   |     -      |     -     |   15.56  \n","   2    |  1300   |   0.110628   |     -      |     -     |   15.57  \n","   2    |  1320   |   0.105893   |     -      |     -     |   15.59  \n","   2    |  1340   |   0.065875   |     -      |     -     |   15.58  \n","   2    |  1360   |   0.084668   |     -      |     -     |   15.57  \n","   2    |  1380   |   0.089533   |     -      |     -     |   15.57  \n","   2    |  1400   |   0.114049   |     -      |     -     |   15.54  \n","   2    |  1420   |   0.116351   |     -      |     -     |   15.54  \n","   2    |  1440   |   0.148490   |     -      |     -     |   15.55  \n","   2    |  1460   |   0.093270   |     -      |     -     |   15.57  \n","   2    |  1480   |   0.133040   |     -      |     -     |   15.59  \n","   2    |  1500   |   0.140362   |     -      |     -     |   15.57  \n","   2    |  1520   |   0.066968   |     -      |     -     |   15.59  \n","   2    |  1540   |   0.098816   |     -      |     -     |   15.59  \n","   2    |  1560   |   0.150233   |     -      |     -     |   15.56  \n","   2    |  1580   |   0.154806   |     -      |     -     |   15.59  \n","   2    |  1600   |   0.156596   |     -      |     -     |   15.57  \n","   2    |  1620   |   0.050560   |     -      |     -     |   15.60  \n","   2    |  1640   |   0.064541   |     -      |     -     |   15.60  \n","   2    |  1660   |   0.078240   |     -      |     -     |   15.58  \n","   2    |  1680   |   0.080212   |     -      |     -     |   15.60  \n","   2    |  1700   |   0.100486   |     -      |     -     |   15.59  \n","   2    |  1720   |   0.193381   |     -      |     -     |   15.58  \n","   2    |  1740   |   0.157503   |     -      |     -     |   15.58  \n","   2    |  1760   |   0.100217   |     -      |     -     |   15.60  \n","   2    |  1780   |   0.146174   |     -      |     -     |   15.58  \n","   2    |  1800   |   0.068996   |     -      |     -     |   15.61  \n","   2    |  1820   |   0.126815   |     -      |     -     |   15.55  \n","   2    |  1840   |   0.120652   |     -      |     -     |   15.58  \n","   2    |  1860   |   0.111145   |     -      |     -     |   15.59  \n","   2    |  1880   |   0.068827   |     -      |     -     |   15.58  \n","   2    |  1900   |   0.096908   |     -      |     -     |   15.57  \n","   2    |  1920   |   0.103231   |     -      |     -     |   15.55  \n","   2    |  1940   |   0.109930   |     -      |     -     |   15.55  \n","   2    |  1960   |   0.118596   |     -      |     -     |   15.57  \n","   2    |  1980   |   0.139314   |     -      |     -     |   15.57  \n","   2    |  2000   |   0.078551   |     -      |     -     |   15.58  \n","   2    |  2020   |   0.072275   |     -      |     -     |   15.57  \n","   2    |  2040   |   0.148843   |     -      |     -     |   15.59  \n","   2    |  2060   |   0.096284   |     -      |     -     |   15.56  \n","   2    |  2080   |   0.150116   |     -      |     -     |   15.57  \n","   2    |  2100   |   0.124336   |     -      |     -     |   15.57  \n","   2    |  2120   |   0.095436   |     -      |     -     |   15.56  \n","   2    |  2140   |   0.146888   |     -      |     -     |   15.59  \n","   2    |  2160   |   0.085248   |     -      |     -     |   15.56  \n","   2    |  2180   |   0.140649   |     -      |     -     |   15.57  \n","   2    |  2200   |   0.107459   |     -      |     -     |   15.56  \n","   2    |  2220   |   0.113845   |     -      |     -     |   15.58  \n","   2    |  2240   |   0.144126   |     -      |     -     |   15.56  \n","   2    |  2260   |   0.112768   |     -      |     -     |   15.57  \n","   2    |  2280   |   0.053661   |     -      |     -     |   15.55  \n","   2    |  2300   |   0.120597   |     -      |     -     |   15.58  \n","   2    |  2320   |   0.095238   |     -      |     -     |   15.58  \n","   2    |  2340   |   0.086824   |     -      |     -     |   15.61  \n","   2    |  2360   |   0.090322   |     -      |     -     |   15.59  \n","   2    |  2380   |   0.154974   |     -      |     -     |   15.61  \n","   2    |  2400   |   0.091322   |     -      |     -     |   15.58  \n","   2    |  2420   |   0.124184   |     -      |     -     |   15.55  \n","   2    |  2440   |   0.103816   |     -      |     -     |   15.58  \n","   2    |  2460   |   0.072203   |     -      |     -     |   15.55  \n","   2    |  2480   |   0.146154   |     -      |     -     |   15.60  \n","   2    |  2500   |   0.085054   |     -      |     -     |   15.56  \n","   2    |  2520   |   0.130964   |     -      |     -     |   15.59  \n","   2    |  2540   |   0.084892   |     -      |     -     |   15.57  \n","   2    |  2560   |   0.088661   |     -      |     -     |   15.58  \n","   2    |  2580   |   0.098240   |     -      |     -     |   15.55  \n","   2    |  2600   |   0.117105   |     -      |     -     |   15.58  \n","   2    |  2620   |   0.069226   |     -      |     -     |   15.57  \n","   2    |  2640   |   0.067103   |     -      |     -     |   15.57  \n","   2    |  2660   |   0.124981   |     -      |     -     |   15.56  \n","   2    |  2680   |   0.110120   |     -      |     -     |   15.57  \n","   2    |  2700   |   0.081781   |     -      |     -     |   15.57  \n","   2    |  2720   |   0.043961   |     -      |     -     |   15.57  \n","   2    |  2740   |   0.107510   |     -      |     -     |   15.57  \n","   2    |  2760   |   0.108845   |     -      |     -     |   15.55  \n","   2    |  2780   |   0.061819   |     -      |     -     |   15.58  \n","   2    |  2800   |   0.152750   |     -      |     -     |   15.54  \n","   2    |  2820   |   0.111056   |     -      |     -     |   15.55  \n","   2    |  2840   |   0.101742   |     -      |     -     |   15.57  \n","   2    |  2860   |   0.080443   |     -      |     -     |   15.57  \n","   2    |  2880   |   0.114609   |     -      |     -     |   15.56  \n","   2    |  2900   |   0.073313   |     -      |     -     |   15.58  \n","   2    |  2920   |   0.200078   |     -      |     -     |   15.56  \n","   2    |  2940   |   0.135144   |     -      |     -     |   15.56  \n","   2    |  2960   |   0.164157   |     -      |     -     |   15.56  \n","   2    |  2980   |   0.207420   |     -      |     -     |   15.56  \n","   2    |  3000   |   0.030923   |     -      |     -     |   15.56  \n","   2    |  3020   |   0.126228   |     -      |     -     |   15.60  \n","   2    |  3040   |   0.057650   |     -      |     -     |   15.55  \n","   2    |  3060   |   0.131236   |     -      |     -     |   15.56  \n","   2    |  3080   |   0.077918   |     -      |     -     |   15.58  \n","   2    |  3100   |   0.073832   |     -      |     -     |   15.58  \n","   2    |  3120   |   0.096329   |     -      |     -     |   15.57  \n","   2    |  3140   |   0.114917   |     -      |     -     |   15.57  \n","   2    |  3160   |   0.105228   |     -      |     -     |   15.58  \n","   2    |  3180   |   0.194333   |     -      |     -     |   15.56  \n","   2    |  3200   |   0.121244   |     -      |     -     |   15.57  \n","   2    |  3220   |   0.099806   |     -      |     -     |   15.56  \n","   2    |  3240   |   0.114115   |     -      |     -     |   15.56  \n","   2    |  3260   |   0.085219   |     -      |     -     |   15.55  \n","   2    |  3280   |   0.099404   |     -      |     -     |   15.57  \n","   2    |  3300   |   0.165766   |     -      |     -     |   15.56  \n","   2    |  3320   |   0.055528   |     -      |     -     |   15.55  \n","   2    |  3340   |   0.111936   |     -      |     -     |   15.53  \n","   2    |  3360   |   0.162144   |     -      |     -     |   15.56  \n","   2    |  3380   |   0.098183   |     -      |     -     |   15.57  \n","   2    |  3400   |   0.092809   |     -      |     -     |   15.57  \n","   2    |  3420   |   0.069744   |     -      |     -     |   15.58  \n","   2    |  3440   |   0.157279   |     -      |     -     |   15.59  \n","   2    |  3460   |   0.095527   |     -      |     -     |   15.58  \n","   2    |  3480   |   0.076382   |     -      |     -     |   15.55  \n","   2    |  3500   |   0.121086   |     -      |     -     |   15.51  \n","   2    |  3520   |   0.110409   |     -      |     -     |   15.54  \n","   2    |  3540   |   0.101980   |     -      |     -     |   15.55  \n","   2    |  3560   |   0.121374   |     -      |     -     |   15.59  \n","   2    |  3580   |   0.140156   |     -      |     -     |   15.57  \n","   2    |  3600   |   0.123143   |     -      |     -     |   15.58  \n","   2    |  3620   |   0.068220   |     -      |     -     |   15.58  \n","   2    |  3640   |   0.136834   |     -      |     -     |   15.57  \n","   2    |  3660   |   0.062012   |     -      |     -     |   15.57  \n","   2    |  3680   |   0.071205   |     -      |     -     |   15.58  \n","   2    |  3700   |   0.090972   |     -      |     -     |   15.57  \n","   2    |  3720   |   0.111234   |     -      |     -     |   15.57  \n","   2    |  3740   |   0.055959   |     -      |     -     |   15.60  \n","   2    |  3760   |   0.121522   |     -      |     -     |   15.57  \n","   2    |  3780   |   0.104501   |     -      |     -     |   15.59  \n","   2    |  3800   |   0.148012   |     -      |     -     |   15.56  \n","   2    |  3820   |   0.058784   |     -      |     -     |   15.56  \n","   2    |  3840   |   0.126340   |     -      |     -     |   15.56  \n","   2    |  3860   |   0.108942   |     -      |     -     |   15.53  \n","   2    |  3880   |   0.099617   |     -      |     -     |   15.53  \n","   2    |  3900   |   0.072585   |     -      |     -     |   15.51  \n","   2    |  3920   |   0.091936   |     -      |     -     |   15.53  \n","   2    |  3940   |   0.159469   |     -      |     -     |   15.55  \n","   2    |  3950   |   0.093346   |     -      |     -     |   7.77   \n","----------------------------------------------------------------------\n"]}],"source":["set_seed(42) \n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pi0d0Im5QxvH"},"outputs":[],"source":["import pickle\n","filename = 'trained-distilbert-base-uncased-emotion-without-emoji-BiLSTM.sav'\n","pickle.dump(bert_classifier, open(filename, 'wb'))"]},{"cell_type":"code","source":["# # Loading the model (to avoid retraining in reruns)\n","\n","# import pickle\n","# filename = 'trained-distilbert-base-uncased-emotion-without-emoji-BiLSTM.sav'\n","# f = open(filename, 'rb')\n","# bert_classifier = pickle.load(f)"],"metadata":{"id":"dUCewdSQ522f"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y84FNM8rlMuq"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def bert_predict(model, test_dataloader):\n","    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n","    on the test set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    all_logits = []\n","\n","    # For each batch in our test set...\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        all_logits.append(logits)\n","    \n","    # Concatenate logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    # Apply softmax to calculate probabilities\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaP0b0jclMxY"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score\n","from sklearn.metrics import precision_recall_curve, f1_score\n","\n","def evaluate_roc(probs, y_true):\n","    \"\"\"\n","    - Print AUC and accuracy on the test set\n","    - Plot ROC\n","    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n","    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n","    \"\"\"\n","    preds = probs[:, 1]\n","    fpr, tpr, threshold = roc_curve(y_true, preds)\n","    roc_auc = auc(fpr, tpr)\n","    print(f'AUC: {roc_auc:.4f}')\n","       \n","    # Get accuracy over the test set\n","    y_pred = np.where(preds >= 0.5, 1, 0)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    print(f'Accuracy: {accuracy*100:.2f}%')\n","\n","    #Get Precision and Recall over the test set\n","    precision  = precision_score(y_true, y_pred, average='binary')\n","    print(f'Precision: {precision*100:.2f}%')\n","    recall = recall_score(y_true, y_pred, average='binary')\n","    print(f'Recall: {recall*100:.2f}%')\n","\n","    f1 = f1_score(y_true, y_pred)\n","\n","    \n","    # Plot ROC AUC\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()\n","\n","    print('Distilbert-base-GRU: f1=%.3f ' % (f1))\n","    # plot the precision-recall curves\n","    baseline = len(y_test[y_test==1]) / len(y_test)\n","    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='Baseline')\n","    plt.plot(recall, precision, marker='.', label='Distilbert-base-GRU')\n","    # axis labels\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    # show the legend\n","    plt.legend()\n","    # show the plot\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUH-ejXRlMz9","colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"status":"ok","timestamp":1643995880957,"user_tz":-330,"elapsed":71711,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06482903253928287171"}},"outputId":"0fc73f2c-796e-4b04-eff0-88e8117e58f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["AUC: 0.9939\n","Accuracy: 96.50%\n","Precision: 96.13%\n","Recall: 98.11%\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["# Compute predicted probabilities on the validation set\n","probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yvvheq0qlM3d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643995892805,"user_tz":-330,"elapsed":11859,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06482903253928287171"}},"outputId":"f3f89506-f76b-473b-a3c5-f8e762f20dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing data...\n"]}],"source":["# Run `preprocessing_for_bert` on the test set\n","print('Tokenizing data...')\n","test_inputs, test_masks = preprocessing_for_bert(X_test)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENShAjsVlh0P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643996069817,"user_tz":-330,"elapsed":177033,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06482903253928287171"}},"outputId":"c60bbce2-d744-4cef-e8a1-e95c3c184539"},"outputs":[{"output_type":"stream","name":"stdout","text":["no-negative tweets ratio  0.6285599275435242\n"]}],"source":["# Compute predicted probabilities on the test set\n","probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.5\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","# Number of tweets predicted non-negative\n","print(\"no-negative tweets ratio \", preds.sum()/len(preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nMBgpwvlh3D","colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"status":"ok","timestamp":1643996069818,"user_tz":-330,"elapsed":19,"user":{"displayName":"Yash Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06482903253928287171"}},"outputId":"c71b24a2-810e-4321-8b27-2f63f53b0e95"},"outputs":[{"output_type":"stream","name":"stdout","text":["AUC: 0.9929\n","Accuracy: 96.13%\n","Precision: 95.52%\n","Recall: 98.27%\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["# Evaluate the Bert classifier for unseen test data\n","evaluate_roc(probs, y_test)"]},{"cell_type":"markdown","source":["Dataset 1"],"metadata":{"id":"UbxnxHjbrN7w"}},{"cell_type":"code","source":["X = df1.text.values\n","y = df1.sentiment.values\n","X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n","X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"],"metadata":{"id":"blHSYaz9rWGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify `MAX_LEN`\n","MAX_LEN =  280\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n","val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"],"metadata":{"id":"LHkD3v3DrWJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"metadata":{"id":"PFgGW0g7rWMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(42) \n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"],"metadata":{"id":"C15aScZ7rWQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"],"metadata":{"id":"BV8qdxdorWW3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Tokenizing data...')\n","test_inputs, test_masks = preprocessing_for_bert(X_test)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"],"metadata":{"id":"IWA5L_vXrWZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.5\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","# Number of tweets predicted non-negative\n","print(\"no-negative tweets ratio \", preds.sum()/len(preds))"],"metadata":{"id":"vmU48X_wrWcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the Bert classifier for unseen test data\n","evaluate_roc(probs, y_test)"],"metadata":{"id":"q2J_5WbYrWe3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset 2"],"metadata":{"id":"__XHLDr7sXzY"}},{"cell_type":"code","source":["X = df2.text.values\n","y = df2.sentiment.values\n","X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n","X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"],"metadata":{"id":"KL4fvGwtsfMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify `MAX_LEN`\n","MAX_LEN =  280\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n","val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"],"metadata":{"id":"UWZaHaRHsfMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"metadata":{"id":"E9HVAST2sfMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(42) \n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"],"metadata":{"id":"0DgNVgLYstSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"],"metadata":{"id":"wsQvAkT3stSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Tokenizing data...')\n","test_inputs, test_masks = preprocessing_for_bert(X_test)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"],"metadata":{"id":"Rfb1hZkNstSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.5\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","# Number of tweets predicted non-negative\n","print(\"no-negative tweets ratio \", preds.sum()/len(preds))"],"metadata":{"id":"k9kl_X-NstSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the Bert classifier for unseen test data\n","evaluate_roc(probs, y_test)"],"metadata":{"id":"CIYUYUDxstSg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uCx1zoZpsihF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset 3"],"metadata":{"id":"9P3Sl4TyskJ_"}},{"cell_type":"code","source":["X = df3.text.values\n","y = df3.sentiment.values\n","X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n","X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"],"metadata":{"id":"NJaYb6qVsmgo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify `MAX_LEN`\n","MAX_LEN =  280\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n","val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"],"metadata":{"id":"cDP6Aq1lsmgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"metadata":{"id":"mGH5uKTBsmgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(42) \n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"],"metadata":{"id":"Ohu1X0l4sv_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"],"metadata":{"id":"ehKs_Hoasv_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Tokenizing data...')\n","test_inputs, test_masks = preprocessing_for_bert(X_test)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"],"metadata":{"id":"zWWGeZrEsv_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.5\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","# Number of tweets predicted non-negative\n","print(\"no-negative tweets ratio \", preds.sum()/len(preds))"],"metadata":{"id":"NnilNQbIsv_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the Bert classifier for unseen test data\n","evaluate_roc(probs, y_test)"],"metadata":{"id":"MkPnUPx3sv_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mLr3anZ4siof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset 4"],"metadata":{"id":"EaM6piRas1XO"}},{"cell_type":"code","source":["X = df4.text.values\n","y = df4.sentiment.values\n","X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n","X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"],"metadata":{"id":"RbrKWSI3s91p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify `MAX_LEN`\n","MAX_LEN =  280\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n","val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"],"metadata":{"id":"fxx7ZTm1s91q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"metadata":{"id":"T_vwMKHbs91r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(42) \n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"],"metadata":{"id":"sCo5YrVZs2jU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"],"metadata":{"id":"HUp-hx4vs2jV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Tokenizing data...')\n","test_inputs, test_masks = preprocessing_for_bert(X_test)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"],"metadata":{"id":"D8_J4ZsEs2jV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.5\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","# Number of tweets predicted non-negative\n","print(\"no-negative tweets ratio \", preds.sum()/len(preds))"],"metadata":{"id":"rSoWoSPZs2jW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the Bert classifier for unseen test data\n","evaluate_roc(probs, y_test)"],"metadata":{"id":"qSqZKD1vs2jW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"QFJkGQigsits"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset 5"],"metadata":{"id":"niWG8neUs4No"}},{"cell_type":"code","source":["X = df5.text.values\n","y = df5.sentiment.values\n","X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n","X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"],"metadata":{"id":"SQV4jrVms_90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify `MAX_LEN`\n","MAX_LEN =  280\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n","val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"],"metadata":{"id":"unFyjf8os_91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"metadata":{"id":"OBSt28hOs_92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(42) \n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"],"metadata":{"id":"CyEj0m9ms56o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"],"metadata":{"id":"FJz8jcOQs56p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Tokenizing data...')\n","test_inputs, test_masks = preprocessing_for_bert(X_test)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"],"metadata":{"id":"hx1c_2xbs56q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.5\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","# Number of tweets predicted non-negative\n","print(\"no-negative tweets ratio \", preds.sum()/len(preds))"],"metadata":{"id":"2dz6at6Ys56q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the Bert classifier for unseen test data\n","evaluate_roc(probs, y_test)"],"metadata":{"id":"UGl5A0X0s56q"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Distilbert-BiLSTM-WithoutEmoji.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ec5b15d2ff0d467ca5a3a7268d57b2c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b96edf8d738b4c2c8c3c8978d20eb6cf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a1e4807f84b144d88743feca09743f77","IPY_MODEL_fe1d86ba7dd24c66a0e0a8d0a3ec2db1","IPY_MODEL_78f88b5131bd4bf7be4dcbcfec222002"]}},"b96edf8d738b4c2c8c3c8978d20eb6cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1e4807f84b144d88743feca09743f77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_21ffc32c92fe40ca9f18b96ab1155574","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9724a8d23694fd09a1c5f1bf964030b"}},"fe1d86ba7dd24c66a0e0a8d0a3ec2db1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1576e739cab8417f9381d20ad361ffcb","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":291,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":291,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ed9d674c47745c5b2ed1d91890a6ad5"}},"78f88b5131bd4bf7be4dcbcfec222002":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1d19c6c0928247cca9f79b91802c6f15","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 291/291 [00:00&lt;00:00, 3.72kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d5209c5a08b4bc5b5e9009307989b34"}},"21ffc32c92fe40ca9f18b96ab1155574":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f9724a8d23694fd09a1c5f1bf964030b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1576e739cab8417f9381d20ad361ffcb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2ed9d674c47745c5b2ed1d91890a6ad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d19c6c0928247cca9f79b91802c6f15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1d5209c5a08b4bc5b5e9009307989b34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42d580795a824ff8ae1c9334bccdd237":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cdcc06d014fe418fb525b352bb96be2d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_94ddfdfa6d2f4c26a5aa28b64b93c611","IPY_MODEL_1d6f082a6639424a9db07f1ddf573455","IPY_MODEL_fe04f46822b74a3dbd58c9df82c9459a"]}},"cdcc06d014fe418fb525b352bb96be2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94ddfdfa6d2f4c26a5aa28b64b93c611":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8cf662a2ed634f0098461aeb3f5c3a08","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14151b067a11486597cad988588f16ab"}},"1d6f082a6639424a9db07f1ddf573455":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_958e2f2127754713bba95b7d6f283d74","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":768,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":768,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d934026808154ad89533166d457b2cc0"}},"fe04f46822b74a3dbd58c9df82c9459a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_722008a704ac4e919c2a7c3e6bce43a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 768/768 [00:00&lt;00:00, 21.7kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a29fb0ade805460bab3c4e7a0b82f283"}},"8cf662a2ed634f0098461aeb3f5c3a08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"14151b067a11486597cad988588f16ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"958e2f2127754713bba95b7d6f283d74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d934026808154ad89533166d457b2cc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"722008a704ac4e919c2a7c3e6bce43a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a29fb0ade805460bab3c4e7a0b82f283":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"067455f6b0884594bddb36e8b3436707":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_678910d0099a4f408c4b0ab7e4baedfc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1de91868528e49d78034f50fb9147ff7","IPY_MODEL_39d5d7a7c8cc4e73bb8dfeb543f15aa0","IPY_MODEL_b65590cd324a4f91a3731138a79ca6da"]}},"678910d0099a4f408c4b0ab7e4baedfc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1de91868528e49d78034f50fb9147ff7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_41d298e0ad96495f8b5433211aa460ef","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3c6bfa329e3d415cb619dad3baeb1eb1"}},"39d5d7a7c8cc4e73bb8dfeb543f15aa0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9b05e2ebd542400bb5d024840ed3f272","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf4ada94f68c4a8ebbdbda42b075fde4"}},"b65590cd324a4f91a3731138a79ca6da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d3554d7052604eafa737ab0f95a16da9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 226k/226k [00:00&lt;00:00, 605kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_33a99a40fb8c4e9aab39cad410a67037"}},"41d298e0ad96495f8b5433211aa460ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3c6bfa329e3d415cb619dad3baeb1eb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b05e2ebd542400bb5d024840ed3f272":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cf4ada94f68c4a8ebbdbda42b075fde4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d3554d7052604eafa737ab0f95a16da9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"33a99a40fb8c4e9aab39cad410a67037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4664291be2084e0183452ff9be397124":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c60008adb8e548b4a284cca970bc796e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_16aa356699db43babb6fb3aac6720e0c","IPY_MODEL_9e5f0121dbe242daae06bb0068be6b74","IPY_MODEL_cdaa95d84a074d42a12c9e9ad2b82c03"]}},"c60008adb8e548b4a284cca970bc796e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16aa356699db43babb6fb3aac6720e0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e13cc76ef0a44b4ea07801597a029b42","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7480697222fd4282b454dff91fb9bcae"}},"9e5f0121dbe242daae06bb0068be6b74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fe649716219840bc89e3c099f438a397","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ad02c8340e7f4a5091a177400cb56faf"}},"cdaa95d84a074d42a12c9e9ad2b82c03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_218736a0fe8f46bbac29a563722cab32","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:00&lt;00:00, 2.19kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_895ebb4131644d7b90682687e1d2d8b4"}},"e13cc76ef0a44b4ea07801597a029b42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7480697222fd4282b454dff91fb9bcae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fe649716219840bc89e3c099f438a397":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ad02c8340e7f4a5091a177400cb56faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"218736a0fe8f46bbac29a563722cab32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"895ebb4131644d7b90682687e1d2d8b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9fa3f408c31d4fb5b21267abab850da7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8291c9a9205c4aa3b64b354537969f7e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d2610201c58040c38d2e85aaa707bab6","IPY_MODEL_ff8f883b21ed4347bbcb387bc1defbae","IPY_MODEL_8e90d48f22f84e7ea6e3d3f735dd1e56"]}},"8291c9a9205c4aa3b64b354537969f7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2610201c58040c38d2e85aaa707bab6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_975856f440614a07ae3399209eeb4391","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a03b37e8795949b4864f00f835bcb165"}},"ff8f883b21ed4347bbcb387bc1defbae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_aa945d6dcd004790962e6ed8f74c0fed","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":267875479,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267875479,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0190563d26304024b8516de3c2df6aee"}},"8e90d48f22f84e7ea6e3d3f735dd1e56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a45ee8984c9241c29f80227616740329","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 255M/255M [00:08&lt;00:00, 29.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eff5d73f01ad44b98e552d165c9819a8"}},"975856f440614a07ae3399209eeb4391":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a03b37e8795949b4864f00f835bcb165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa945d6dcd004790962e6ed8f74c0fed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0190563d26304024b8516de3c2df6aee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a45ee8984c9241c29f80227616740329":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eff5d73f01ad44b98e552d165c9819a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}