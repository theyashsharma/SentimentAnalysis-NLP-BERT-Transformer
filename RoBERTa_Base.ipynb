{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa-Base.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment classification with English Twitter Datasets"
      ],
      "metadata": {
        "id": "05KYS_pnO3Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "8_6lr4dbPZNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O2ganP7WOcv-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 ) Loading Datasets"
      ],
      "metadata": {
        "id": "IpJdkXmSPKfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('tweets_sqgames.csv')\n",
        "df1 = df1.loc[:, ['text', 'sentiment']]\n",
        "label_mapping = {\"Positive\": 1, \"Negative\":0}\n",
        "df1 = df1[df1.sentiment != \"Neutral\"]\n",
        "df1[\"sentiment\"] = df1[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "E200bW4uv0hW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('Tweets.csv')\n",
        "df2 = df2.loc[:, ['text', 'airline_sentiment']]\n",
        "df2 = df2.rename(columns = {\"airline_sentiment\":\"sentiment\"})\n",
        "label_mapping = {\"positive\": 1, \"negative\":0}\n",
        "df2 = df2[df2.sentiment != \"neutral\"]\n",
        "df2[\"sentiment\"] = df2[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "9zdyPvyDv0kq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.read_csv('apple-twitter-sentiment-texts.csv')\n",
        "label_mapping = {1: 1, -1:0}\n",
        "df3 = df3[df3.sentiment != 0]\n",
        "df3[\"sentiment\"] = df3[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "wKAkAPNlv0r6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = pd.read_csv('Apple-Twitter-Sentiment-DFE.csv', encoding=\"Latin-1\")\n",
        "label_mapping = {\"5\": 1, \"1\":0}\n",
        "df4 = df4[df4.sentiment != \"3\"]\n",
        "df4 = df4[df4.sentiment != \"not_relevant\"]\n",
        "df4[\"sentiment\"] = df4[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "f-d6W4F4v0vR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = pd.read_csv('Reddit_Data.csv')\n",
        "df5 = df5.rename(columns = {\"clean_comment\":\"text\", \"category\":\"sentiment\"})\n",
        "label_mapping = {1: 1, -1:0}\n",
        "df5 = df5[df5.sentiment != 0]\n",
        "df5[\"sentiment\"] = df5[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "77x4gFaC6Dry"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [df1, df2, df3, df4, df5]\n",
        "merged_df = pd.concat(frames)"
      ],
      "metadata": {
        "id": "K4ADJIznV0Bb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = merged_df.text.values\n",
        "y = merged_df.sentiment.values"
      ],
      "metadata": {
        "id": "M0LtDH9_O1pm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "5H2DVOPHO1sE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 ) Deep Learning Approach"
      ],
      "metadata": {
        "id": "HakjkIbYY-EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oBZeFVSO2AH",
        "outputId": "fbbbea48-0f69-4ca0-c4a8-d0f038e0da90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI790cijO2GE",
        "outputId": "3ccab3e7-8397-486a-dbce-26a53d711923"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel"
      ],
      "metadata": {
        "id": "SLMSbnC5O2DJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
      ],
      "metadata": {
        "id": "xMmgFGdOO2Jh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize unicode encoding\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    #Remove URLs\n",
        "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '<URL>', text)\n",
        "\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "o2TenWCGZw7I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emojis(sent):\n",
        "    text =  emoji.demojize(sent)\n",
        "    text= re.sub(r'(:[!_\\-\\w]+:)', '', text)\n",
        "    return text\n",
        "    \n",
        "def text_preprocessing_no_emojis(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "  \n",
        "    # Remove emojis\n",
        "    text = remove_emojis(text)\n",
        "\n",
        "    return text_preprocessing(text)"
      ],
      "metadata": {
        "id": "Lp6sDjV9L2hQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gklJcuwha7hu",
        "outputId": "49bcc7b7-d70f-4b98-b091-792ca0106008"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "import unicodedata\n",
        "def preprocessing_for_bert(data, version=\"mini\", text_preprocessing_fn = text_preprocessing):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")# if version == \"mini\" else RobertaTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "\n",
        "    # For every sentence...\n",
        "    for i,sent in enumerate(data):\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing_fn(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            padding='max_length',        # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,     # Return attention mask\n",
        "            truncation = True \n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "8fjl5AftaQU7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCm99bITa5hL",
        "outputId": "8306e165-3889-41e5-f636-dc47e1c9854a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  When life hits and the same time poverty strikes you\n",
            "Gong Yoo : Lets play a game \n",
            "#SquidGame #Netflix https://t.co/Cx7ifmZ8cN\n",
            "Token IDs:  [0, 1779, 301, 2323, 8, 5, 276, 86, 5263, 5315, 47, 36135, 854, 3036, 4832, 40702, 310, 10, 177, 849, 38378, 808, 20178, 849, 29675, 1205, 640, 90, 4, 876, 73, 347, 1178, 406, 1594, 119, 1301, 398, 438, 487, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Tokenizing data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "U7iTT16LbLni"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False, version=\"mini\"):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in = 768\n",
        "        H, D_out = 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            #nn.Dropout(p=0.1),\n",
        "            #nn.Linear(768, D_in),\n",
        "            #nn.Tanh(),\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.roberta(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32gvSQ9pb9Fr",
        "outputId": "76c118eb-6687-44cd-c38c-3ed0ccc611c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.82 ms, sys: 14 µs, total: 7.84 ms\n",
            "Wall time: 7.51 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.optim import SparseAdam, Adam\n",
        "def initialize_model(epochs=4, version=\"base\"):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False, version=version)\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = torch.optim.AdamW(params=list(bert_classifier.parameters()),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "metadata": {
        "id": "z7RwU_UBPHUh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "       \n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "DwlxCFVUPHXN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "I0K_YbfyO1TR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560ccf76-871b-4f33-923b-76573a97cbdd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.578079   |     -      |     -     |   29.87  \n",
            "   1    |   40    |   0.585208   |     -      |     -     |   28.50  \n",
            "   1    |   60    |   0.552128   |     -      |     -     |   28.50  \n",
            "   1    |   80    |   0.519892   |     -      |     -     |   28.75  \n",
            "   1    |   100   |   0.464720   |     -      |     -     |   28.73  \n",
            "   1    |   120   |   0.485911   |     -      |     -     |   28.76  \n",
            "   1    |   140   |   0.456612   |     -      |     -     |   28.73  \n",
            "   1    |   160   |   0.436678   |     -      |     -     |   28.77  \n",
            "   1    |   180   |   0.490235   |     -      |     -     |   28.79  \n",
            "   1    |   200   |   0.405700   |     -      |     -     |   28.63  \n",
            "   1    |   220   |   0.372858   |     -      |     -     |   28.64  \n",
            "   1    |   240   |   0.347105   |     -      |     -     |   28.74  \n",
            "   1    |   260   |   0.363220   |     -      |     -     |   28.83  \n",
            "   1    |   280   |   0.484628   |     -      |     -     |   28.75  \n",
            "   1    |   300   |   0.385059   |     -      |     -     |   28.61  \n",
            "   1    |   320   |   0.356557   |     -      |     -     |   28.70  \n",
            "   1    |   340   |   0.253473   |     -      |     -     |   28.56  \n",
            "   1    |   360   |   0.388436   |     -      |     -     |   28.71  \n",
            "   1    |   380   |   0.276495   |     -      |     -     |   28.65  \n",
            "   1    |   400   |   0.422582   |     -      |     -     |   28.73  \n",
            "   1    |   420   |   0.379061   |     -      |     -     |   28.61  \n",
            "   1    |   440   |   0.440459   |     -      |     -     |   28.64  \n",
            "   1    |   460   |   0.323736   |     -      |     -     |   28.69  \n",
            "   1    |   480   |   0.388988   |     -      |     -     |   28.80  \n",
            "   1    |   500   |   0.410311   |     -      |     -     |   28.79  \n",
            "   1    |   520   |   0.355253   |     -      |     -     |   28.76  \n",
            "   1    |   540   |   0.404672   |     -      |     -     |   28.73  \n",
            "   1    |   560   |   0.294795   |     -      |     -     |   28.70  \n",
            "   1    |   580   |   0.317251   |     -      |     -     |   28.69  \n",
            "   1    |   600   |   0.343021   |     -      |     -     |   28.72  \n",
            "   1    |   620   |   0.262225   |     -      |     -     |   28.67  \n",
            "   1    |   640   |   0.285558   |     -      |     -     |   28.76  \n",
            "   1    |   660   |   0.368486   |     -      |     -     |   28.80  \n",
            "   1    |   680   |   0.290944   |     -      |     -     |   28.61  \n",
            "   1    |   700   |   0.263616   |     -      |     -     |   28.74  \n",
            "   1    |   720   |   0.413140   |     -      |     -     |   28.76  \n",
            "   1    |   740   |   0.266559   |     -      |     -     |   28.73  \n",
            "   1    |   760   |   0.411975   |     -      |     -     |   28.66  \n",
            "   1    |   780   |   0.349216   |     -      |     -     |   28.76  \n",
            "   1    |   800   |   0.279153   |     -      |     -     |   28.59  \n",
            "   1    |   820   |   0.278632   |     -      |     -     |   28.69  \n",
            "   1    |   840   |   0.291857   |     -      |     -     |   28.60  \n",
            "   1    |   860   |   0.289447   |     -      |     -     |   28.84  \n",
            "   1    |   880   |   0.268499   |     -      |     -     |   28.60  \n",
            "   1    |   900   |   0.323950   |     -      |     -     |   28.73  \n",
            "   1    |   920   |   0.335011   |     -      |     -     |   28.60  \n",
            "   1    |   940   |   0.315998   |     -      |     -     |   28.80  \n",
            "   1    |   960   |   0.299002   |     -      |     -     |   28.79  \n",
            "   1    |   980   |   0.289716   |     -      |     -     |   28.67  \n",
            "   1    |  1000   |   0.288887   |     -      |     -     |   28.58  \n",
            "   1    |  1020   |   0.254584   |     -      |     -     |   28.69  \n",
            "   1    |  1040   |   0.238976   |     -      |     -     |   28.63  \n",
            "   1    |  1060   |   0.225833   |     -      |     -     |   28.73  \n",
            "   1    |  1080   |   0.265552   |     -      |     -     |   28.70  \n",
            "   1    |  1100   |   0.254897   |     -      |     -     |   28.77  \n",
            "   1    |  1120   |   0.364790   |     -      |     -     |   28.69  \n",
            "   1    |  1140   |   0.228331   |     -      |     -     |   28.62  \n",
            "   1    |  1160   |   0.236819   |     -      |     -     |   28.66  \n",
            "   1    |  1180   |   0.280971   |     -      |     -     |   28.69  \n",
            "   1    |  1200   |   0.233490   |     -      |     -     |   28.61  \n",
            "   1    |  1220   |   0.397392   |     -      |     -     |   28.73  \n",
            "   1    |  1240   |   0.292377   |     -      |     -     |   28.71  \n",
            "   1    |  1260   |   0.296181   |     -      |     -     |   28.74  \n",
            "   1    |  1280   |   0.192598   |     -      |     -     |   28.71  \n",
            "   1    |  1300   |   0.268413   |     -      |     -     |   28.64  \n",
            "   1    |  1320   |   0.270575   |     -      |     -     |   28.54  \n",
            "   1    |  1340   |   0.246252   |     -      |     -     |   28.69  \n",
            "   1    |  1360   |   0.199251   |     -      |     -     |   28.72  \n",
            "   1    |  1380   |   0.316931   |     -      |     -     |   28.70  \n",
            "   1    |  1400   |   0.253226   |     -      |     -     |   28.69  \n",
            "   1    |  1420   |   0.291294   |     -      |     -     |   28.58  \n",
            "   1    |  1440   |   0.292476   |     -      |     -     |   28.72  \n",
            "   1    |  1460   |   0.217605   |     -      |     -     |   28.92  \n",
            "   1    |  1480   |   0.316209   |     -      |     -     |   28.69  \n",
            "   1    |  1500   |   0.225445   |     -      |     -     |   28.65  \n",
            "   1    |  1520   |   0.222095   |     -      |     -     |   28.71  \n",
            "   1    |  1540   |   0.205375   |     -      |     -     |   28.66  \n",
            "   1    |  1560   |   0.262915   |     -      |     -     |   28.71  \n",
            "   1    |  1580   |   0.328152   |     -      |     -     |   28.73  \n",
            "   1    |  1600   |   0.238334   |     -      |     -     |   28.67  \n",
            "   1    |  1620   |   0.248419   |     -      |     -     |   28.75  \n",
            "   1    |  1640   |   0.259653   |     -      |     -     |   28.62  \n",
            "   1    |  1660   |   0.275120   |     -      |     -     |   28.70  \n",
            "   1    |  1680   |   0.180972   |     -      |     -     |   28.74  \n",
            "   1    |  1700   |   0.228925   |     -      |     -     |   28.67  \n",
            "   1    |  1720   |   0.262049   |     -      |     -     |   28.65  \n",
            "   1    |  1740   |   0.238803   |     -      |     -     |   28.72  \n",
            "   1    |  1760   |   0.261286   |     -      |     -     |   28.67  \n",
            "   1    |  1780   |   0.293699   |     -      |     -     |   28.79  \n",
            "   1    |  1800   |   0.289914   |     -      |     -     |   28.71  \n",
            "   1    |  1820   |   0.231897   |     -      |     -     |   28.68  \n",
            "   1    |  1840   |   0.359211   |     -      |     -     |   28.68  \n",
            "   1    |  1860   |   0.251099   |     -      |     -     |   28.54  \n",
            "   1    |  1880   |   0.256382   |     -      |     -     |   28.56  \n",
            "   1    |  1900   |   0.209365   |     -      |     -     |   28.72  \n",
            "   1    |  1920   |   0.181754   |     -      |     -     |   28.61  \n",
            "   1    |  1940   |   0.263917   |     -      |     -     |   28.77  \n",
            "   1    |  1960   |   0.202182   |     -      |     -     |   28.72  \n",
            "   1    |  1980   |   0.303883   |     -      |     -     |   28.73  \n",
            "   1    |  2000   |   0.225094   |     -      |     -     |   28.76  \n",
            "   1    |  2020   |   0.268150   |     -      |     -     |   28.75  \n",
            "   1    |  2040   |   0.184664   |     -      |     -     |   28.66  \n",
            "   1    |  2060   |   0.228577   |     -      |     -     |   28.71  \n",
            "   1    |  2080   |   0.263855   |     -      |     -     |   28.77  \n",
            "   1    |  2100   |   0.215423   |     -      |     -     |   28.70  \n",
            "   1    |  2120   |   0.243752   |     -      |     -     |   28.64  \n",
            "   1    |  2140   |   0.180663   |     -      |     -     |   28.66  \n",
            "   1    |  2160   |   0.205695   |     -      |     -     |   28.80  \n",
            "   1    |  2180   |   0.244107   |     -      |     -     |   28.63  \n",
            "   1    |  2200   |   0.236981   |     -      |     -     |   28.58  \n",
            "   1    |  2220   |   0.198098   |     -      |     -     |   28.66  \n",
            "   1    |  2240   |   0.250473   |     -      |     -     |   28.74  \n",
            "   1    |  2260   |   0.207476   |     -      |     -     |   28.77  \n",
            "   1    |  2280   |   0.287370   |     -      |     -     |   28.56  \n",
            "   1    |  2300   |   0.269514   |     -      |     -     |   28.84  \n",
            "   1    |  2320   |   0.275518   |     -      |     -     |   28.68  \n",
            "   1    |  2340   |   0.182419   |     -      |     -     |   28.62  \n",
            "   1    |  2360   |   0.214052   |     -      |     -     |   28.66  \n",
            "   1    |  2380   |   0.240541   |     -      |     -     |   28.74  \n",
            "   1    |  2400   |   0.306330   |     -      |     -     |   28.77  \n",
            "   1    |  2420   |   0.251730   |     -      |     -     |   28.69  \n",
            "   1    |  2440   |   0.171947   |     -      |     -     |   28.58  \n",
            "   1    |  2460   |   0.205467   |     -      |     -     |   28.95  \n",
            "   1    |  2480   |   0.280398   |     -      |     -     |   28.82  \n",
            "   1    |  2500   |   0.184974   |     -      |     -     |   28.67  \n",
            "   1    |  2520   |   0.266159   |     -      |     -     |   28.69  \n",
            "   1    |  2540   |   0.271910   |     -      |     -     |   28.74  \n",
            "   1    |  2560   |   0.274773   |     -      |     -     |   28.67  \n",
            "   1    |  2580   |   0.239935   |     -      |     -     |   28.62  \n",
            "   1    |  2600   |   0.256437   |     -      |     -     |   28.58  \n",
            "   1    |  2620   |   0.176246   |     -      |     -     |   28.71  \n",
            "   1    |  2640   |   0.185939   |     -      |     -     |   28.57  \n",
            "   1    |  2660   |   0.238943   |     -      |     -     |   28.65  \n",
            "   1    |  2680   |   0.198275   |     -      |     -     |   28.78  \n",
            "   1    |  2700   |   0.212183   |     -      |     -     |   28.75  \n",
            "   1    |  2720   |   0.186538   |     -      |     -     |   28.85  \n",
            "   1    |  2740   |   0.245807   |     -      |     -     |   28.66  \n",
            "   1    |  2760   |   0.274421   |     -      |     -     |   28.76  \n",
            "   1    |  2780   |   0.218473   |     -      |     -     |   28.37  \n",
            "   1    |  2800   |   0.256800   |     -      |     -     |   28.77  \n",
            "   1    |  2820   |   0.249705   |     -      |     -     |   28.83  \n",
            "   1    |  2840   |   0.332890   |     -      |     -     |   28.83  \n",
            "   1    |  2860   |   0.321327   |     -      |     -     |   28.81  \n",
            "   1    |  2880   |   0.180101   |     -      |     -     |   28.82  \n",
            "   1    |  2900   |   0.201516   |     -      |     -     |   28.83  \n",
            "   1    |  2920   |   0.181576   |     -      |     -     |   28.80  \n",
            "   1    |  2940   |   0.242570   |     -      |     -     |   28.82  \n",
            "   1    |  2960   |   0.238443   |     -      |     -     |   28.84  \n",
            "   1    |  2980   |   0.163142   |     -      |     -     |   28.80  \n",
            "   1    |  3000   |   0.245028   |     -      |     -     |   28.73  \n",
            "   1    |  3020   |   0.272153   |     -      |     -     |   28.75  \n",
            "   1    |  3040   |   0.242896   |     -      |     -     |   28.76  \n",
            "   1    |  3060   |   0.249812   |     -      |     -     |   28.80  \n",
            "   1    |  3080   |   0.193267   |     -      |     -     |   28.75  \n",
            "   1    |  3100   |   0.169215   |     -      |     -     |   28.76  \n",
            "   1    |  3120   |   0.318760   |     -      |     -     |   28.75  \n",
            "   1    |  3140   |   0.160264   |     -      |     -     |   28.72  \n",
            "   1    |  3160   |   0.203216   |     -      |     -     |   28.73  \n",
            "   1    |  3180   |   0.219714   |     -      |     -     |   28.74  \n",
            "   1    |  3200   |   0.192105   |     -      |     -     |   28.71  \n",
            "   1    |  3220   |   0.172371   |     -      |     -     |   28.78  \n",
            "   1    |  3240   |   0.229584   |     -      |     -     |   28.74  \n",
            "   1    |  3260   |   0.167216   |     -      |     -     |   28.73  \n",
            "   1    |  3280   |   0.240539   |     -      |     -     |   28.76  \n",
            "   1    |  3300   |   0.191850   |     -      |     -     |   28.79  \n",
            "   1    |  3320   |   0.193369   |     -      |     -     |   28.80  \n",
            "   1    |  3340   |   0.141339   |     -      |     -     |   28.80  \n",
            "   1    |  3360   |   0.299984   |     -      |     -     |   28.75  \n",
            "   1    |  3380   |   0.222426   |     -      |     -     |   28.74  \n",
            "   1    |  3400   |   0.244244   |     -      |     -     |   28.76  \n",
            "   1    |  3420   |   0.260776   |     -      |     -     |   28.78  \n",
            "   1    |  3440   |   0.216611   |     -      |     -     |   28.77  \n",
            "   1    |  3460   |   0.234082   |     -      |     -     |   28.76  \n",
            "   1    |  3480   |   0.195096   |     -      |     -     |   28.81  \n",
            "   1    |  3500   |   0.192447   |     -      |     -     |   28.78  \n",
            "   1    |  3520   |   0.266033   |     -      |     -     |   28.80  \n",
            "   1    |  3540   |   0.162713   |     -      |     -     |   28.81  \n",
            "   1    |  3560   |   0.204966   |     -      |     -     |   28.80  \n",
            "   1    |  3580   |   0.188294   |     -      |     -     |   28.79  \n",
            "   1    |  3600   |   0.204605   |     -      |     -     |   28.80  \n",
            "   1    |  3620   |   0.180369   |     -      |     -     |   28.79  \n",
            "   1    |  3640   |   0.114081   |     -      |     -     |   28.81  \n",
            "   1    |  3660   |   0.248456   |     -      |     -     |   28.79  \n",
            "   1    |  3680   |   0.217131   |     -      |     -     |   28.78  \n",
            "   1    |  3700   |   0.278573   |     -      |     -     |   28.78  \n",
            "   1    |  3720   |   0.243524   |     -      |     -     |   28.78  \n",
            "   1    |  3740   |   0.182958   |     -      |     -     |   28.78  \n",
            "   1    |  3760   |   0.252313   |     -      |     -     |   28.79  \n",
            "   1    |  3780   |   0.283457   |     -      |     -     |   28.81  \n",
            "   1    |  3800   |   0.222865   |     -      |     -     |   28.81  \n",
            "   1    |  3820   |   0.192486   |     -      |     -     |   28.79  \n",
            "   1    |  3840   |   0.187800   |     -      |     -     |   28.82  \n",
            "   1    |  3860   |   0.199835   |     -      |     -     |   28.83  \n",
            "   1    |  3880   |   0.198851   |     -      |     -     |   28.81  \n",
            "   1    |  3900   |   0.179051   |     -      |     -     |   28.85  \n",
            "   1    |  3920   |   0.203347   |     -      |     -     |   28.83  \n",
            "   1    |  3940   |   0.129351   |     -      |     -     |   28.79  \n",
            "   1    |  3950   |   0.200438   |     -      |     -     |   14.41  \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.269030   |  0.182961  |   93.79   |  5904.99 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.078973   |     -      |     -     |   30.26  \n",
            "   2    |   40    |   0.121352   |     -      |     -     |   28.82  \n",
            "   2    |   60    |   0.149858   |     -      |     -     |   28.86  \n",
            "   2    |   80    |   0.200030   |     -      |     -     |   28.80  \n",
            "   2    |   100   |   0.206917   |     -      |     -     |   28.84  \n",
            "   2    |   120   |   0.127136   |     -      |     -     |   28.84  \n",
            "   2    |   140   |   0.183079   |     -      |     -     |   28.79  \n",
            "   2    |   160   |   0.183069   |     -      |     -     |   28.85  \n",
            "   2    |   180   |   0.163096   |     -      |     -     |   28.84  \n",
            "   2    |   200   |   0.140122   |     -      |     -     |   28.83  \n",
            "   2    |   220   |   0.218689   |     -      |     -     |   28.82  \n",
            "   2    |   240   |   0.105264   |     -      |     -     |   28.82  \n",
            "   2    |   260   |   0.209564   |     -      |     -     |   28.78  \n",
            "   2    |   280   |   0.125306   |     -      |     -     |   28.81  \n",
            "   2    |   300   |   0.190713   |     -      |     -     |   28.80  \n",
            "   2    |   320   |   0.133238   |     -      |     -     |   28.83  \n",
            "   2    |   340   |   0.082618   |     -      |     -     |   28.78  \n",
            "   2    |   360   |   0.170009   |     -      |     -     |   28.82  \n",
            "   2    |   380   |   0.144370   |     -      |     -     |   28.81  \n",
            "   2    |   400   |   0.178923   |     -      |     -     |   28.79  \n",
            "   2    |   420   |   0.183473   |     -      |     -     |   28.81  \n",
            "   2    |   440   |   0.160783   |     -      |     -     |   28.81  \n",
            "   2    |   460   |   0.121280   |     -      |     -     |   28.78  \n",
            "   2    |   480   |   0.119164   |     -      |     -     |   28.75  \n",
            "   2    |   500   |   0.149096   |     -      |     -     |   28.71  \n",
            "   2    |   520   |   0.163039   |     -      |     -     |   28.78  \n",
            "   2    |   540   |   0.202916   |     -      |     -     |   28.77  \n",
            "   2    |   560   |   0.092630   |     -      |     -     |   29.07  \n",
            "   2    |   580   |   0.168710   |     -      |     -     |   28.49  \n",
            "   2    |   600   |   0.121994   |     -      |     -     |   28.69  \n",
            "   2    |   620   |   0.121446   |     -      |     -     |   28.80  \n",
            "   2    |   640   |   0.175213   |     -      |     -     |   28.70  \n",
            "   2    |   660   |   0.115692   |     -      |     -     |   28.78  \n",
            "   2    |   680   |   0.169836   |     -      |     -     |   28.72  \n",
            "   2    |   700   |   0.132605   |     -      |     -     |   28.75  \n",
            "   2    |   720   |   0.091224   |     -      |     -     |   28.77  \n",
            "   2    |   740   |   0.145836   |     -      |     -     |   28.76  \n",
            "   2    |   760   |   0.203818   |     -      |     -     |   28.81  \n",
            "   2    |   780   |   0.266808   |     -      |     -     |   28.75  \n",
            "   2    |   800   |   0.125986   |     -      |     -     |   28.74  \n",
            "   2    |   820   |   0.081257   |     -      |     -     |   28.77  \n",
            "   2    |   840   |   0.286831   |     -      |     -     |   28.81  \n",
            "   2    |   860   |   0.093389   |     -      |     -     |   28.76  \n",
            "   2    |   880   |   0.184034   |     -      |     -     |   28.78  \n",
            "   2    |   900   |   0.152249   |     -      |     -     |   28.78  \n",
            "   2    |   920   |   0.157104   |     -      |     -     |   28.75  \n",
            "   2    |   940   |   0.198253   |     -      |     -     |   28.73  \n",
            "   2    |   960   |   0.184808   |     -      |     -     |   28.75  \n",
            "   2    |   980   |   0.246146   |     -      |     -     |   28.76  \n",
            "   2    |  1000   |   0.143046   |     -      |     -     |   28.74  \n",
            "   2    |  1020   |   0.094839   |     -      |     -     |   28.79  \n",
            "   2    |  1040   |   0.112368   |     -      |     -     |   28.75  \n",
            "   2    |  1060   |   0.161314   |     -      |     -     |   28.72  \n",
            "   2    |  1080   |   0.201690   |     -      |     -     |   28.74  \n",
            "   2    |  1100   |   0.108194   |     -      |     -     |   28.71  \n",
            "   2    |  1120   |   0.112706   |     -      |     -     |   28.74  \n",
            "   2    |  1140   |   0.079791   |     -      |     -     |   28.73  \n",
            "   2    |  1160   |   0.200113   |     -      |     -     |   28.72  \n",
            "   2    |  1180   |   0.100551   |     -      |     -     |   28.73  \n",
            "   2    |  1200   |   0.102418   |     -      |     -     |   28.73  \n",
            "   2    |  1220   |   0.078046   |     -      |     -     |   28.81  \n",
            "   2    |  1240   |   0.234169   |     -      |     -     |   28.75  \n",
            "   2    |  1260   |   0.102752   |     -      |     -     |   28.73  \n",
            "   2    |  1280   |   0.102649   |     -      |     -     |   28.72  \n",
            "   2    |  1300   |   0.077892   |     -      |     -     |   28.70  \n",
            "   2    |  1320   |   0.232069   |     -      |     -     |   28.75  \n",
            "   2    |  1340   |   0.141552   |     -      |     -     |   28.77  \n",
            "   2    |  1360   |   0.193875   |     -      |     -     |   28.76  \n",
            "   2    |  1380   |   0.090237   |     -      |     -     |   28.74  \n",
            "   2    |  1400   |   0.138305   |     -      |     -     |   28.77  \n",
            "   2    |  1420   |   0.078895   |     -      |     -     |   28.72  \n",
            "   2    |  1440   |   0.124587   |     -      |     -     |   28.75  \n",
            "   2    |  1460   |   0.199866   |     -      |     -     |   28.73  \n",
            "   2    |  1480   |   0.127663   |     -      |     -     |   28.70  \n",
            "   2    |  1500   |   0.098646   |     -      |     -     |   28.76  \n",
            "   2    |  1520   |   0.100249   |     -      |     -     |   28.74  \n",
            "   2    |  1540   |   0.187915   |     -      |     -     |   28.71  \n",
            "   2    |  1560   |   0.055696   |     -      |     -     |   28.72  \n",
            "   2    |  1580   |   0.107737   |     -      |     -     |   28.77  \n",
            "   2    |  1600   |   0.200246   |     -      |     -     |   28.75  \n",
            "   2    |  1620   |   0.121189   |     -      |     -     |   28.73  \n",
            "   2    |  1640   |   0.114397   |     -      |     -     |   28.75  \n",
            "   2    |  1660   |   0.081074   |     -      |     -     |   28.77  \n",
            "   2    |  1680   |   0.108942   |     -      |     -     |   28.78  \n",
            "   2    |  1700   |   0.150651   |     -      |     -     |   28.77  \n",
            "   2    |  1720   |   0.190302   |     -      |     -     |   28.78  \n",
            "   2    |  1740   |   0.101917   |     -      |     -     |   28.77  \n",
            "   2    |  1760   |   0.113528   |     -      |     -     |   28.73  \n",
            "   2    |  1780   |   0.175817   |     -      |     -     |   28.72  \n",
            "   2    |  1800   |   0.163621   |     -      |     -     |   28.75  \n",
            "   2    |  1820   |   0.205459   |     -      |     -     |   28.70  \n",
            "   2    |  1840   |   0.114213   |     -      |     -     |   28.71  \n",
            "   2    |  1860   |   0.186992   |     -      |     -     |   28.72  \n",
            "   2    |  1880   |   0.201417   |     -      |     -     |   28.78  \n",
            "   2    |  1900   |   0.155171   |     -      |     -     |   28.72  \n",
            "   2    |  1920   |   0.136974   |     -      |     -     |   28.74  \n",
            "   2    |  1940   |   0.178003   |     -      |     -     |   28.77  \n",
            "   2    |  1960   |   0.095338   |     -      |     -     |   28.75  \n",
            "   2    |  1980   |   0.134324   |     -      |     -     |   28.72  \n",
            "   2    |  2000   |   0.103166   |     -      |     -     |   28.70  \n",
            "   2    |  2020   |   0.193454   |     -      |     -     |   28.72  \n",
            "   2    |  2040   |   0.126212   |     -      |     -     |   28.72  \n",
            "   2    |  2060   |   0.107755   |     -      |     -     |   28.72  \n",
            "   2    |  2080   |   0.125734   |     -      |     -     |   28.75  \n",
            "   2    |  2100   |   0.200906   |     -      |     -     |   28.75  \n",
            "   2    |  2120   |   0.114012   |     -      |     -     |   28.72  \n",
            "   2    |  2140   |   0.182137   |     -      |     -     |   28.75  \n",
            "   2    |  2160   |   0.095750   |     -      |     -     |   28.75  \n",
            "   2    |  2180   |   0.133116   |     -      |     -     |   28.74  \n",
            "   2    |  2200   |   0.096046   |     -      |     -     |   28.74  \n",
            "   2    |  2220   |   0.126899   |     -      |     -     |   28.77  \n",
            "   2    |  2240   |   0.070849   |     -      |     -     |   28.74  \n",
            "   2    |  2260   |   0.116870   |     -      |     -     |   28.74  \n",
            "   2    |  2280   |   0.102379   |     -      |     -     |   28.74  \n",
            "   2    |  2300   |   0.135641   |     -      |     -     |   28.74  \n",
            "   2    |  2320   |   0.118355   |     -      |     -     |   28.73  \n",
            "   2    |  2340   |   0.150554   |     -      |     -     |   28.72  \n",
            "   2    |  2360   |   0.102007   |     -      |     -     |   28.73  \n",
            "   2    |  2380   |   0.150214   |     -      |     -     |   28.78  \n",
            "   2    |  2400   |   0.122708   |     -      |     -     |   28.79  \n",
            "   2    |  2420   |   0.116182   |     -      |     -     |   28.82  \n",
            "   2    |  2440   |   0.122876   |     -      |     -     |   28.79  \n",
            "   2    |  2460   |   0.249449   |     -      |     -     |   28.79  \n",
            "   2    |  2480   |   0.104778   |     -      |     -     |   28.80  \n",
            "   2    |  2500   |   0.079720   |     -      |     -     |   28.78  \n",
            "   2    |  2520   |   0.109714   |     -      |     -     |   28.77  \n",
            "   2    |  2540   |   0.150383   |     -      |     -     |   28.78  \n",
            "   2    |  2560   |   0.080555   |     -      |     -     |   28.75  \n",
            "   2    |  2580   |   0.166424   |     -      |     -     |   28.73  \n",
            "   2    |  2600   |   0.089598   |     -      |     -     |   28.78  \n",
            "   2    |  2620   |   0.090835   |     -      |     -     |   28.77  \n",
            "   2    |  2640   |   0.098190   |     -      |     -     |   28.81  \n",
            "   2    |  2660   |   0.157374   |     -      |     -     |   28.79  \n",
            "   2    |  2680   |   0.094528   |     -      |     -     |   28.80  \n",
            "   2    |  2700   |   0.086850   |     -      |     -     |   28.80  \n",
            "   2    |  2720   |   0.144820   |     -      |     -     |   28.83  \n",
            "   2    |  2740   |   0.135014   |     -      |     -     |   28.79  \n",
            "   2    |  2760   |   0.158651   |     -      |     -     |   28.81  \n",
            "   2    |  2780   |   0.118230   |     -      |     -     |   28.72  \n",
            "   2    |  2800   |   0.127019   |     -      |     -     |   28.75  \n",
            "   2    |  2820   |   0.146404   |     -      |     -     |   28.76  \n",
            "   2    |  2840   |   0.113959   |     -      |     -     |   28.75  \n",
            "   2    |  2860   |   0.156514   |     -      |     -     |   28.78  \n",
            "   2    |  2880   |   0.106336   |     -      |     -     |   28.73  \n",
            "   2    |  2900   |   0.138405   |     -      |     -     |   28.75  \n",
            "   2    |  2920   |   0.078246   |     -      |     -     |   28.74  \n",
            "   2    |  2940   |   0.083851   |     -      |     -     |   28.73  \n",
            "   2    |  2960   |   0.164808   |     -      |     -     |   28.74  \n",
            "   2    |  2980   |   0.123939   |     -      |     -     |   28.73  \n",
            "   2    |  3000   |   0.049389   |     -      |     -     |   28.71  \n",
            "   2    |  3020   |   0.193472   |     -      |     -     |   28.71  \n",
            "   2    |  3040   |   0.178186   |     -      |     -     |   28.72  \n",
            "   2    |  3060   |   0.166261   |     -      |     -     |   28.73  \n",
            "   2    |  3080   |   0.161797   |     -      |     -     |   28.77  \n",
            "   2    |  3100   |   0.133151   |     -      |     -     |   28.78  \n",
            "   2    |  3120   |   0.157337   |     -      |     -     |   28.74  \n",
            "   2    |  3140   |   0.116723   |     -      |     -     |   28.76  \n",
            "   2    |  3160   |   0.051245   |     -      |     -     |   28.77  \n",
            "   2    |  3180   |   0.117717   |     -      |     -     |   28.75  \n",
            "   2    |  3200   |   0.056017   |     -      |     -     |   28.75  \n",
            "   2    |  3220   |   0.122606   |     -      |     -     |   28.75  \n",
            "   2    |  3240   |   0.169680   |     -      |     -     |   28.75  \n",
            "   2    |  3260   |   0.167344   |     -      |     -     |   28.77  \n",
            "   2    |  3280   |   0.175366   |     -      |     -     |   28.71  \n",
            "   2    |  3300   |   0.202119   |     -      |     -     |   28.73  \n",
            "   2    |  3320   |   0.071285   |     -      |     -     |   28.77  \n",
            "   2    |  3340   |   0.137976   |     -      |     -     |   28.72  \n",
            "   2    |  3360   |   0.180926   |     -      |     -     |   28.78  \n",
            "   2    |  3380   |   0.103546   |     -      |     -     |   28.76  \n",
            "   2    |  3400   |   0.150799   |     -      |     -     |   28.76  \n",
            "   2    |  3420   |   0.098790   |     -      |     -     |   28.73  \n",
            "   2    |  3440   |   0.074340   |     -      |     -     |   28.74  \n",
            "   2    |  3460   |   0.163960   |     -      |     -     |   28.76  \n",
            "   2    |  3480   |   0.096760   |     -      |     -     |   28.76  \n",
            "   2    |  3500   |   0.124696   |     -      |     -     |   28.79  \n",
            "   2    |  3520   |   0.135788   |     -      |     -     |   28.76  \n",
            "   2    |  3540   |   0.125937   |     -      |     -     |   28.61  \n",
            "   2    |  3560   |   0.124970   |     -      |     -     |   28.54  \n",
            "   2    |  3580   |   0.039234   |     -      |     -     |   28.53  \n",
            "   2    |  3600   |   0.098092   |     -      |     -     |   28.63  \n",
            "   2    |  3620   |   0.063199   |     -      |     -     |   28.74  \n",
            "   2    |  3640   |   0.166569   |     -      |     -     |   28.29  \n",
            "   2    |  3660   |   0.163791   |     -      |     -     |   28.45  \n",
            "   2    |  3680   |   0.111224   |     -      |     -     |   28.42  \n",
            "   2    |  3700   |   0.056364   |     -      |     -     |   28.53  \n",
            "   2    |  3720   |   0.070689   |     -      |     -     |   28.52  \n",
            "   2    |  3740   |   0.122532   |     -      |     -     |   28.53  \n",
            "   2    |  3760   |   0.102348   |     -      |     -     |   28.54  \n",
            "   2    |  3780   |   0.094043   |     -      |     -     |   28.50  \n",
            "   2    |  3800   |   0.129052   |     -      |     -     |   28.52  \n",
            "   2    |  3820   |   0.094649   |     -      |     -     |   28.51  \n",
            "   2    |  3840   |   0.122416   |     -      |     -     |   28.53  \n",
            "   2    |  3860   |   0.063594   |     -      |     -     |   28.51  \n",
            "   2    |  3880   |   0.116206   |     -      |     -     |   28.53  \n",
            "   2    |  3900   |   0.111202   |     -      |     -     |   28.52  \n",
            "   2    |  3920   |   0.107746   |     -      |     -     |   28.60  \n",
            "   2    |  3940   |   0.122119   |     -      |     -     |   28.74  \n",
            "   2    |  3950   |   0.143621   |     -      |     -     |   14.37  \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.134927   |  0.211098  |   94.99   |  5907.57 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename = 'trained-twitter-roberta-base-sentiment.sav'\n",
        "pickle.dump(bert_classifier, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "Pi0d0Im5QxvH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "Y84FNM8rlMuq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    #Get Precision and Recall over the test set\n",
        "    precision  = precision_score(y_true, y_pred, average='binary')\n",
        "    print(f'Precision: {precision*100:.2f}%')\n",
        "    recall = recall_score(y_true, y_pred, average='binary')\n",
        "    print(f'Recall: {recall*100:.2f}%')\n",
        "\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "    print('Distilbert-base-GRU: f1=%.3f ' % (f1))\n",
        "    # plot the precision-recall curves\n",
        "    baseline = len(y_test[y_test==1]) / len(y_test)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='Baseline')\n",
        "    plt.plot(recall, precision, marker='.', label='Distilbert-base-GRU')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "BaP0b0jclMxY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted probabilities on the validation set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "MUH-ejXRlMz9",
        "outputId": "695089ac-b346-405c-d9da-d330fbd9393e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9898\n",
            "Accuracy: 94.98%\n",
            "Precision: 95.16%\n",
            "Recall: 95.64%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run `preprocessing_for_bert` on the test set\n",
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "Yvvheq0qlM3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4bbb79-c7d3-4e8b-dde3-8a249b316d7e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "ENShAjsVlh0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd9d26a-a83a-4c50-8684-4326ebb7e579"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no-negative tweets ratio  0.557485336825921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "4nMBgpwvlh3D",
        "outputId": "43289cfc-dbf9-4e20-aac5-c32170628918"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9917\n",
            "Accuracy: 95.55%\n",
            "Precision: 95.83%\n",
            "Recall: 96.18%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 1"
      ],
      "metadata": {
        "id": "UbxnxHjbrN7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.text.values\n",
        "y = df1.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "blHSYaz9rWGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "LHkD3v3DrWJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "PFgGW0g7rWMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "C15aScZ7rWQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "BV8qdxdorWW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "IWA5L_vXrWZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "vmU48X_wrWcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "q2J_5WbYrWe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 2"
      ],
      "metadata": {
        "id": "__XHLDr7sXzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df2.text.values\n",
        "y = df2.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "KL4fvGwtsfMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "UWZaHaRHsfMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "E9HVAST2sfMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "0DgNVgLYstSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "wsQvAkT3stSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "Rfb1hZkNstSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "k9kl_X-NstSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "CIYUYUDxstSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCx1zoZpsihF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 3"
      ],
      "metadata": {
        "id": "9P3Sl4TyskJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df3.text.values\n",
        "y = df3.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "NJaYb6qVsmgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "cDP6Aq1lsmgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "mGH5uKTBsmgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "Ohu1X0l4sv_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "ehKs_Hoasv_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "zWWGeZrEsv_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "NnilNQbIsv_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "MkPnUPx3sv_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mLr3anZ4siof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 4"
      ],
      "metadata": {
        "id": "EaM6piRas1XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df4.text.values\n",
        "y = df4.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "RbrKWSI3s91p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "fxx7ZTm1s91q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "T_vwMKHbs91r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "sCo5YrVZs2jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "HUp-hx4vs2jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "D8_J4ZsEs2jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "rSoWoSPZs2jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "qSqZKD1vs2jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QFJkGQigsits"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 5"
      ],
      "metadata": {
        "id": "niWG8neUs4No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df5.text.values\n",
        "y = df5.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "SQV4jrVms_90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "unFyjf8os_91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "OBSt28hOs_92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "CyEj0m9ms56o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "FJz8jcOQs56p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "hx1c_2xbs56q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "2dz6at6Ys56q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "UGl5A0X0s56q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}