{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa-Base-BiGRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment classification with English Twitter Datasets"
      ],
      "metadata": {
        "id": "05KYS_pnO3Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "8_6lr4dbPZNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O2ganP7WOcv-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 ) Loading Datasets"
      ],
      "metadata": {
        "id": "IpJdkXmSPKfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('tweets_sqgames.csv')\n",
        "df1 = df1.loc[:, ['text', 'sentiment']]\n",
        "label_mapping = {\"Positive\": 1, \"Negative\":0}\n",
        "df1 = df1[df1.sentiment != \"Neutral\"]\n",
        "df1[\"sentiment\"] = df1[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "E200bW4uv0hW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('Tweets.csv')\n",
        "df2 = df2.loc[:, ['text', 'airline_sentiment']]\n",
        "df2 = df2.rename(columns = {\"airline_sentiment\":\"sentiment\"})\n",
        "label_mapping = {\"positive\": 1, \"negative\":0}\n",
        "df2 = df2[df2.sentiment != \"neutral\"]\n",
        "df2[\"sentiment\"] = df2[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "9zdyPvyDv0kq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.read_csv('apple-twitter-sentiment-texts.csv')\n",
        "label_mapping = {1: 1, -1:0}\n",
        "df3 = df3[df3.sentiment != 0]\n",
        "df3[\"sentiment\"] = df3[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "wKAkAPNlv0r6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = pd.read_csv('Apple-Twitter-Sentiment-DFE.csv', encoding=\"Latin-1\")\n",
        "label_mapping = {\"5\": 1, \"1\":0}\n",
        "df4 = df4[df4.sentiment != \"3\"]\n",
        "df4 = df4[df4.sentiment != \"not_relevant\"]\n",
        "df4[\"sentiment\"] = df4[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "f-d6W4F4v0vR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = pd.read_csv('Reddit_Data.csv')\n",
        "df5 = df5.rename(columns = {\"clean_comment\":\"text\", \"category\":\"sentiment\"})\n",
        "label_mapping = {1: 1, -1:0}\n",
        "df5 = df5[df5.sentiment != 0]\n",
        "df5[\"sentiment\"] = df5[\"sentiment\"].map(label_mapping)"
      ],
      "metadata": {
        "id": "77x4gFaC6Dry"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [df1, df2, df3, df4, df5]\n",
        "merged_df = pd.concat(frames)"
      ],
      "metadata": {
        "id": "hIXnRwDnUPu5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = merged_df.text.values\n",
        "y = merged_df.sentiment.values"
      ],
      "metadata": {
        "id": "M0LtDH9_O1pm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "5H2DVOPHO1sE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 ) Deep Learning Approach"
      ],
      "metadata": {
        "id": "HakjkIbYY-EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oBZeFVSO2AH",
        "outputId": "2e5fba70-3ce7-4f4a-a912-0e85536d5ea8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI790cijO2GE",
        "outputId": "a4ef4496-4b5c-4344-91ec-65315066fa70"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel"
      ],
      "metadata": {
        "id": "SLMSbnC5O2DJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
      ],
      "metadata": {
        "id": "xMmgFGdOO2Jh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize unicode encoding\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    #Remove URLs\n",
        "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '<URL>', text)\n",
        "\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "o2TenWCGZw7I"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emojis(sent):\n",
        "    text =  emoji.demojize(sent)\n",
        "    text= re.sub(r'(:[!_\\-\\w]+:)', '', text)\n",
        "    return text\n",
        "    \n",
        "def text_preprocessing_no_emojis(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "  \n",
        "    # Remove emojis\n",
        "    text = remove_emojis(text)\n",
        "\n",
        "    return text_preprocessing(text)"
      ],
      "metadata": {
        "id": "Lp6sDjV9L2hQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gklJcuwha7hu",
        "outputId": "8829c168-73c0-4378-df95-abd5544a32ef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "import unicodedata\n",
        "def preprocessing_for_bert(data, version=\"mini\", text_preprocessing_fn = text_preprocessing):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")# if version == \"mini\" else RobertaTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "\n",
        "    # For every sentence...\n",
        "    for i,sent in enumerate(data):\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing_fn(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            padding='max_length',        # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,     # Return attention mask\n",
        "            truncation = True \n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "8fjl5AftaQU7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCm99bITa5hL",
        "outputId": "3e4eff06-1467-42e5-d088-070dc64dc45d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  When life hits and the same time poverty strikes you\n",
            "Gong Yoo : Lets play a game \n",
            "#SquidGame #Netflix https://t.co/Cx7ifmZ8cN\n",
            "Token IDs:  [0, 1779, 301, 2323, 8, 5, 276, 86, 5263, 5315, 47, 36135, 854, 3036, 4832, 40702, 310, 10, 177, 849, 38378, 808, 20178, 849, 29675, 1205, 640, 90, 4, 876, 73, 347, 1178, 406, 1594, 119, 1301, 398, 438, 487, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Tokenizing data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "U7iTT16LbLni"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False, version=\"mini\"):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in = 768\n",
        "        H, D_out = 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        self.gru = nn.GRU(768, 768, bidirectional = True)\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.roberta(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32gvSQ9pb9Fr",
        "outputId": "0b4575a9-8873-49a6-a775-2742dfc47d18"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 47 µs, sys: 1e+03 ns, total: 48 µs\n",
            "Wall time: 51.3 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.optim import SparseAdam, Adam\n",
        "def initialize_model(epochs=4, version=\"base\"):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False, version=version)\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = torch.optim.AdamW(params=list(bert_classifier.parameters()),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "metadata": {
        "id": "z7RwU_UBPHUh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "       \n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "DwlxCFVUPHXN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "I0K_YbfyO1TR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47614395-cd62-4f27-f7ca-7b446e1e8706"
      },
      "execution_count": 46,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.627650   |     -      |     -     |   16.26  \n",
            "   1    |   40    |   0.554219   |     -      |     -     |   15.57  \n",
            "   1    |   60    |   0.532630   |     -      |     -     |   15.41  \n",
            "   1    |   80    |   0.496979   |     -      |     -     |   15.42  \n",
            "   1    |   100   |   0.508310   |     -      |     -     |   15.45  \n",
            "   1    |   120   |   0.418414   |     -      |     -     |   15.55  \n",
            "   1    |   140   |   0.500084   |     -      |     -     |   15.62  \n",
            "   1    |   160   |   0.505437   |     -      |     -     |   15.74  \n",
            "   1    |   180   |   0.466823   |     -      |     -     |   15.73  \n",
            "   1    |   200   |   0.448147   |     -      |     -     |   15.77  \n",
            "   1    |   220   |   0.352958   |     -      |     -     |   15.87  \n",
            "   1    |   240   |   0.386025   |     -      |     -     |   15.82  \n",
            "   1    |   260   |   0.430838   |     -      |     -     |   15.86  \n",
            "   1    |   280   |   0.486632   |     -      |     -     |   15.88  \n",
            "   1    |   300   |   0.432226   |     -      |     -     |   15.90  \n",
            "   1    |   320   |   0.375193   |     -      |     -     |   15.88  \n",
            "   1    |   340   |   0.380624   |     -      |     -     |   15.89  \n",
            "   1    |   360   |   0.328945   |     -      |     -     |   15.89  \n",
            "   1    |   380   |   0.390887   |     -      |     -     |   15.88  \n",
            "   1    |   400   |   0.404549   |     -      |     -     |   15.87  \n",
            "   1    |   420   |   0.352820   |     -      |     -     |   15.87  \n",
            "   1    |   440   |   0.341143   |     -      |     -     |   15.87  \n",
            "   1    |   460   |   0.366274   |     -      |     -     |   15.84  \n",
            "   1    |   480   |   0.324783   |     -      |     -     |   15.84  \n",
            "   1    |   500   |   0.331312   |     -      |     -     |   15.83  \n",
            "   1    |   520   |   0.389643   |     -      |     -     |   15.84  \n",
            "   1    |   540   |   0.379316   |     -      |     -     |   15.85  \n",
            "   1    |   560   |   0.322504   |     -      |     -     |   15.85  \n",
            "   1    |   580   |   0.307607   |     -      |     -     |   15.84  \n",
            "   1    |   600   |   0.305609   |     -      |     -     |   15.98  \n",
            "   1    |   620   |   0.273308   |     -      |     -     |   16.08  \n",
            "   1    |   640   |   0.370037   |     -      |     -     |   15.86  \n",
            "   1    |   660   |   0.363147   |     -      |     -     |   15.85  \n",
            "   1    |   680   |   0.317084   |     -      |     -     |   15.86  \n",
            "   1    |   700   |   0.299732   |     -      |     -     |   15.87  \n",
            "   1    |   720   |   0.296641   |     -      |     -     |   15.88  \n",
            "   1    |   740   |   0.349473   |     -      |     -     |   15.86  \n",
            "   1    |   760   |   0.326598   |     -      |     -     |   15.87  \n",
            "   1    |   780   |   0.320921   |     -      |     -     |   15.88  \n",
            "   1    |   800   |   0.280097   |     -      |     -     |   15.85  \n",
            "   1    |   820   |   0.373293   |     -      |     -     |   15.87  \n",
            "   1    |   840   |   0.266208   |     -      |     -     |   15.84  \n",
            "   1    |   860   |   0.316048   |     -      |     -     |   15.86  \n",
            "   1    |   880   |   0.301353   |     -      |     -     |   15.83  \n",
            "   1    |   900   |   0.254640   |     -      |     -     |   15.82  \n",
            "   1    |   920   |   0.315771   |     -      |     -     |   15.83  \n",
            "   1    |   940   |   0.227775   |     -      |     -     |   15.85  \n",
            "   1    |   960   |   0.283181   |     -      |     -     |   15.85  \n",
            "   1    |   980   |   0.312507   |     -      |     -     |   15.82  \n",
            "   1    |  1000   |   0.231315   |     -      |     -     |   15.85  \n",
            "   1    |  1020   |   0.306059   |     -      |     -     |   15.84  \n",
            "   1    |  1040   |   0.304341   |     -      |     -     |   15.84  \n",
            "   1    |  1060   |   0.327274   |     -      |     -     |   15.84  \n",
            "   1    |  1080   |   0.198464   |     -      |     -     |   15.86  \n",
            "   1    |  1100   |   0.307585   |     -      |     -     |   15.85  \n",
            "   1    |  1120   |   0.272166   |     -      |     -     |   15.86  \n",
            "   1    |  1140   |   0.238036   |     -      |     -     |   15.85  \n",
            "   1    |  1160   |   0.237133   |     -      |     -     |   15.87  \n",
            "   1    |  1180   |   0.281790   |     -      |     -     |   15.86  \n",
            "   1    |  1200   |   0.227218   |     -      |     -     |   15.86  \n",
            "   1    |  1220   |   0.253789   |     -      |     -     |   15.85  \n",
            "   1    |  1240   |   0.324608   |     -      |     -     |   15.85  \n",
            "   1    |  1260   |   0.278989   |     -      |     -     |   15.82  \n",
            "   1    |  1280   |   0.210390   |     -      |     -     |   15.84  \n",
            "   1    |  1300   |   0.239503   |     -      |     -     |   15.85  \n",
            "   1    |  1320   |   0.192019   |     -      |     -     |   15.83  \n",
            "   1    |  1340   |   0.316314   |     -      |     -     |   15.85  \n",
            "   1    |  1360   |   0.344458   |     -      |     -     |   15.86  \n",
            "   1    |  1380   |   0.246841   |     -      |     -     |   15.83  \n",
            "   1    |  1400   |   0.258565   |     -      |     -     |   15.85  \n",
            "   1    |  1420   |   0.258221   |     -      |     -     |   15.83  \n",
            "   1    |  1440   |   0.296930   |     -      |     -     |   15.85  \n",
            "   1    |  1460   |   0.230750   |     -      |     -     |   15.85  \n",
            "   1    |  1480   |   0.335606   |     -      |     -     |   15.86  \n",
            "   1    |  1500   |   0.345523   |     -      |     -     |   15.84  \n",
            "   1    |  1520   |   0.249377   |     -      |     -     |   15.82  \n",
            "   1    |  1540   |   0.303962   |     -      |     -     |   15.84  \n",
            "   1    |  1560   |   0.225056   |     -      |     -     |   15.85  \n",
            "   1    |  1580   |   0.227607   |     -      |     -     |   15.83  \n",
            "   1    |  1600   |   0.308291   |     -      |     -     |   15.83  \n",
            "   1    |  1620   |   0.276032   |     -      |     -     |   15.85  \n",
            "   1    |  1640   |   0.268931   |     -      |     -     |   15.85  \n",
            "   1    |  1660   |   0.379990   |     -      |     -     |   15.84  \n",
            "   1    |  1680   |   0.276442   |     -      |     -     |   15.86  \n",
            "   1    |  1700   |   0.176468   |     -      |     -     |   15.85  \n",
            "   1    |  1720   |   0.298732   |     -      |     -     |   15.85  \n",
            "   1    |  1740   |   0.213292   |     -      |     -     |   15.85  \n",
            "   1    |  1760   |   0.249543   |     -      |     -     |   15.85  \n",
            "   1    |  1780   |   0.261256   |     -      |     -     |   15.86  \n",
            "   1    |  1800   |   0.244479   |     -      |     -     |   15.88  \n",
            "   1    |  1820   |   0.221294   |     -      |     -     |   15.87  \n",
            "   1    |  1840   |   0.281855   |     -      |     -     |   15.84  \n",
            "   1    |  1860   |   0.276449   |     -      |     -     |   15.85  \n",
            "   1    |  1880   |   0.212875   |     -      |     -     |   15.84  \n",
            "   1    |  1900   |   0.257132   |     -      |     -     |   15.86  \n",
            "   1    |  1920   |   0.193655   |     -      |     -     |   15.86  \n",
            "   1    |  1940   |   0.250345   |     -      |     -     |   15.86  \n",
            "   1    |  1960   |   0.166997   |     -      |     -     |   15.85  \n",
            "   1    |  1980   |   0.261679   |     -      |     -     |   15.85  \n",
            "   1    |  2000   |   0.244827   |     -      |     -     |   15.84  \n",
            "   1    |  2020   |   0.223642   |     -      |     -     |   15.84  \n",
            "   1    |  2040   |   0.245759   |     -      |     -     |   15.84  \n",
            "   1    |  2060   |   0.233417   |     -      |     -     |   15.83  \n",
            "   1    |  2080   |   0.272422   |     -      |     -     |   15.83  \n",
            "   1    |  2100   |   0.246062   |     -      |     -     |   15.83  \n",
            "   1    |  2120   |   0.227017   |     -      |     -     |   15.83  \n",
            "   1    |  2140   |   0.331414   |     -      |     -     |   15.86  \n",
            "   1    |  2160   |   0.238424   |     -      |     -     |   15.85  \n",
            "   1    |  2180   |   0.239911   |     -      |     -     |   15.84  \n",
            "   1    |  2200   |   0.182412   |     -      |     -     |   15.84  \n",
            "   1    |  2220   |   0.247360   |     -      |     -     |   15.86  \n",
            "   1    |  2240   |   0.293337   |     -      |     -     |   15.88  \n",
            "   1    |  2260   |   0.211781   |     -      |     -     |   15.84  \n",
            "   1    |  2280   |   0.204845   |     -      |     -     |   15.86  \n",
            "   1    |  2300   |   0.255327   |     -      |     -     |   15.84  \n",
            "   1    |  2320   |   0.269994   |     -      |     -     |   15.87  \n",
            "   1    |  2340   |   0.188465   |     -      |     -     |   15.84  \n",
            "   1    |  2360   |   0.235380   |     -      |     -     |   15.82  \n",
            "   1    |  2380   |   0.260011   |     -      |     -     |   15.84  \n",
            "   1    |  2400   |   0.276459   |     -      |     -     |   15.83  \n",
            "   1    |  2420   |   0.202252   |     -      |     -     |   15.83  \n",
            "   1    |  2440   |   0.182514   |     -      |     -     |   15.83  \n",
            "   1    |  2460   |   0.214356   |     -      |     -     |   15.82  \n",
            "   1    |  2480   |   0.290685   |     -      |     -     |   15.84  \n",
            "   1    |  2500   |   0.183729   |     -      |     -     |   15.85  \n",
            "   1    |  2520   |   0.163116   |     -      |     -     |   15.85  \n",
            "   1    |  2540   |   0.403945   |     -      |     -     |   15.85  \n",
            "   1    |  2560   |   0.260170   |     -      |     -     |   15.85  \n",
            "   1    |  2580   |   0.273751   |     -      |     -     |   15.87  \n",
            "   1    |  2600   |   0.236942   |     -      |     -     |   15.86  \n",
            "   1    |  2620   |   0.230926   |     -      |     -     |   15.85  \n",
            "   1    |  2640   |   0.223844   |     -      |     -     |   15.86  \n",
            "   1    |  2660   |   0.210788   |     -      |     -     |   15.89  \n",
            "   1    |  2680   |   0.189473   |     -      |     -     |   15.86  \n",
            "   1    |  2700   |   0.330226   |     -      |     -     |   15.85  \n",
            "   1    |  2720   |   0.211090   |     -      |     -     |   15.86  \n",
            "   1    |  2740   |   0.135569   |     -      |     -     |   15.84  \n",
            "   1    |  2760   |   0.232154   |     -      |     -     |   15.86  \n",
            "   1    |  2780   |   0.313804   |     -      |     -     |   15.84  \n",
            "   1    |  2800   |   0.204911   |     -      |     -     |   15.83  \n",
            "   1    |  2820   |   0.260731   |     -      |     -     |   15.84  \n",
            "   1    |  2840   |   0.197049   |     -      |     -     |   15.85  \n",
            "   1    |  2860   |   0.234728   |     -      |     -     |   15.84  \n",
            "   1    |  2880   |   0.233348   |     -      |     -     |   15.84  \n",
            "   1    |  2900   |   0.252665   |     -      |     -     |   15.84  \n",
            "   1    |  2920   |   0.232796   |     -      |     -     |   15.85  \n",
            "   1    |  2940   |   0.186280   |     -      |     -     |   15.88  \n",
            "   1    |  2960   |   0.190195   |     -      |     -     |   15.82  \n",
            "   1    |  2980   |   0.242322   |     -      |     -     |   15.83  \n",
            "   1    |  3000   |   0.191989   |     -      |     -     |   15.86  \n",
            "   1    |  3020   |   0.244045   |     -      |     -     |   15.84  \n",
            "   1    |  3040   |   0.207005   |     -      |     -     |   15.84  \n",
            "   1    |  3060   |   0.225144   |     -      |     -     |   15.85  \n",
            "   1    |  3080   |   0.279304   |     -      |     -     |   15.85  \n",
            "   1    |  3100   |   0.229619   |     -      |     -     |   15.84  \n",
            "   1    |  3120   |   0.239673   |     -      |     -     |   15.84  \n",
            "   1    |  3140   |   0.136661   |     -      |     -     |   15.84  \n",
            "   1    |  3160   |   0.191797   |     -      |     -     |   15.86  \n",
            "   1    |  3180   |   0.198381   |     -      |     -     |   15.86  \n",
            "   1    |  3200   |   0.192824   |     -      |     -     |   15.85  \n",
            "   1    |  3220   |   0.211549   |     -      |     -     |   15.83  \n",
            "   1    |  3240   |   0.188865   |     -      |     -     |   15.85  \n",
            "   1    |  3260   |   0.251683   |     -      |     -     |   15.86  \n",
            "   1    |  3280   |   0.192039   |     -      |     -     |   15.84  \n",
            "   1    |  3300   |   0.245777   |     -      |     -     |   15.85  \n",
            "   1    |  3320   |   0.185652   |     -      |     -     |   15.83  \n",
            "   1    |  3340   |   0.250833   |     -      |     -     |   15.87  \n",
            "   1    |  3360   |   0.178796   |     -      |     -     |   15.85  \n",
            "   1    |  3380   |   0.226652   |     -      |     -     |   15.87  \n",
            "   1    |  3400   |   0.210132   |     -      |     -     |   15.85  \n",
            "   1    |  3420   |   0.247279   |     -      |     -     |   15.85  \n",
            "   1    |  3440   |   0.149155   |     -      |     -     |   15.85  \n",
            "   1    |  3460   |   0.187888   |     -      |     -     |   15.85  \n",
            "   1    |  3480   |   0.293596   |     -      |     -     |   15.84  \n",
            "   1    |  3500   |   0.265221   |     -      |     -     |   15.84  \n",
            "   1    |  3520   |   0.179369   |     -      |     -     |   15.83  \n",
            "   1    |  3540   |   0.226541   |     -      |     -     |   15.82  \n",
            "   1    |  3560   |   0.242857   |     -      |     -     |   15.83  \n",
            "   1    |  3580   |   0.196911   |     -      |     -     |   15.84  \n",
            "   1    |  3600   |   0.131330   |     -      |     -     |   15.85  \n",
            "   1    |  3620   |   0.208317   |     -      |     -     |   15.84  \n",
            "   1    |  3640   |   0.200437   |     -      |     -     |   15.83  \n",
            "   1    |  3660   |   0.199619   |     -      |     -     |   15.85  \n",
            "   1    |  3680   |   0.221336   |     -      |     -     |   15.86  \n",
            "   1    |  3700   |   0.214330   |     -      |     -     |   15.83  \n",
            "   1    |  3720   |   0.114791   |     -      |     -     |   15.82  \n",
            "   1    |  3740   |   0.159342   |     -      |     -     |   15.84  \n",
            "   1    |  3760   |   0.219237   |     -      |     -     |   15.85  \n",
            "   1    |  3780   |   0.144417   |     -      |     -     |   15.84  \n",
            "   1    |  3800   |   0.275303   |     -      |     -     |   15.84  \n",
            "   1    |  3820   |   0.161360   |     -      |     -     |   15.85  \n",
            "   1    |  3840   |   0.166777   |     -      |     -     |   15.84  \n",
            "   1    |  3860   |   0.226289   |     -      |     -     |   15.85  \n",
            "   1    |  3880   |   0.121868   |     -      |     -     |   15.85  \n",
            "   1    |  3900   |   0.212089   |     -      |     -     |   15.83  \n",
            "   1    |  3920   |   0.211763   |     -      |     -     |   15.86  \n",
            "   1    |  3940   |   0.206205   |     -      |     -     |   15.86  \n",
            "   1    |  3950   |   0.152221   |     -      |     -     |   7.93   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.269624   |  0.191031  |   93.61   |  3245.02 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.144340   |     -      |     -     |   16.65  \n",
            "   2    |   40    |   0.175297   |     -      |     -     |   15.88  \n",
            "   2    |   60    |   0.162177   |     -      |     -     |   15.88  \n",
            "   2    |   80    |   0.119889   |     -      |     -     |   15.88  \n",
            "   2    |   100   |   0.064139   |     -      |     -     |   15.85  \n",
            "   2    |   120   |   0.120295   |     -      |     -     |   15.82  \n",
            "   2    |   140   |   0.154316   |     -      |     -     |   15.83  \n",
            "   2    |   160   |   0.234591   |     -      |     -     |   15.83  \n",
            "   2    |   180   |   0.169258   |     -      |     -     |   15.84  \n",
            "   2    |   200   |   0.086236   |     -      |     -     |   15.84  \n",
            "   2    |   220   |   0.277573   |     -      |     -     |   15.84  \n",
            "   2    |   240   |   0.169232   |     -      |     -     |   15.83  \n",
            "   2    |   260   |   0.122755   |     -      |     -     |   15.84  \n",
            "   2    |   280   |   0.135894   |     -      |     -     |   15.84  \n",
            "   2    |   300   |   0.195008   |     -      |     -     |   15.84  \n",
            "   2    |   320   |   0.159433   |     -      |     -     |   15.85  \n",
            "   2    |   340   |   0.120625   |     -      |     -     |   15.85  \n",
            "   2    |   360   |   0.138006   |     -      |     -     |   15.84  \n",
            "   2    |   380   |   0.124083   |     -      |     -     |   15.83  \n",
            "   2    |   400   |   0.154480   |     -      |     -     |   15.83  \n",
            "   2    |   420   |   0.184080   |     -      |     -     |   15.82  \n",
            "   2    |   440   |   0.261999   |     -      |     -     |   15.86  \n",
            "   2    |   460   |   0.180135   |     -      |     -     |   15.83  \n",
            "   2    |   480   |   0.157805   |     -      |     -     |   15.85  \n",
            "   2    |   500   |   0.146075   |     -      |     -     |   15.83  \n",
            "   2    |   520   |   0.144379   |     -      |     -     |   15.85  \n",
            "   2    |   540   |   0.186778   |     -      |     -     |   15.85  \n",
            "   2    |   560   |   0.112600   |     -      |     -     |   15.85  \n",
            "   2    |   580   |   0.103174   |     -      |     -     |   15.84  \n",
            "   2    |   600   |   0.185450   |     -      |     -     |   15.85  \n",
            "   2    |   620   |   0.127711   |     -      |     -     |   15.86  \n",
            "   2    |   640   |   0.124801   |     -      |     -     |   15.83  \n",
            "   2    |   660   |   0.123661   |     -      |     -     |   15.82  \n",
            "   2    |   680   |   0.190382   |     -      |     -     |   15.84  \n",
            "   2    |   700   |   0.157156   |     -      |     -     |   15.83  \n",
            "   2    |   720   |   0.186149   |     -      |     -     |   15.83  \n",
            "   2    |   740   |   0.123830   |     -      |     -     |   15.84  \n",
            "   2    |   760   |   0.148713   |     -      |     -     |   15.82  \n",
            "   2    |   780   |   0.212669   |     -      |     -     |   15.85  \n",
            "   2    |   800   |   0.166225   |     -      |     -     |   15.84  \n",
            "   2    |   820   |   0.201675   |     -      |     -     |   15.84  \n",
            "   2    |   840   |   0.079653   |     -      |     -     |   15.83  \n",
            "   2    |   860   |   0.161847   |     -      |     -     |   15.85  \n",
            "   2    |   880   |   0.062732   |     -      |     -     |   15.83  \n",
            "   2    |   900   |   0.177843   |     -      |     -     |   15.83  \n",
            "   2    |   920   |   0.227256   |     -      |     -     |   15.86  \n",
            "   2    |   940   |   0.116422   |     -      |     -     |   15.85  \n",
            "   2    |   960   |   0.090660   |     -      |     -     |   15.85  \n",
            "   2    |   980   |   0.118175   |     -      |     -     |   15.83  \n",
            "   2    |  1000   |   0.178009   |     -      |     -     |   15.86  \n",
            "   2    |  1020   |   0.165407   |     -      |     -     |   15.87  \n",
            "   2    |  1040   |   0.251189   |     -      |     -     |   15.84  \n",
            "   2    |  1060   |   0.091749   |     -      |     -     |   15.83  \n",
            "   2    |  1080   |   0.127159   |     -      |     -     |   15.83  \n",
            "   2    |  1100   |   0.125980   |     -      |     -     |   15.86  \n",
            "   2    |  1120   |   0.112803   |     -      |     -     |   15.85  \n",
            "   2    |  1140   |   0.084341   |     -      |     -     |   15.84  \n",
            "   2    |  1160   |   0.139077   |     -      |     -     |   15.85  \n",
            "   2    |  1180   |   0.196478   |     -      |     -     |   15.83  \n",
            "   2    |  1200   |   0.268314   |     -      |     -     |   15.84  \n",
            "   2    |  1220   |   0.161839   |     -      |     -     |   15.84  \n",
            "   2    |  1240   |   0.132489   |     -      |     -     |   15.86  \n",
            "   2    |  1260   |   0.131346   |     -      |     -     |   15.83  \n",
            "   2    |  1280   |   0.098145   |     -      |     -     |   15.84  \n",
            "   2    |  1300   |   0.129091   |     -      |     -     |   15.85  \n",
            "   2    |  1320   |   0.083571   |     -      |     -     |   15.82  \n",
            "   2    |  1340   |   0.125088   |     -      |     -     |   15.83  \n",
            "   2    |  1360   |   0.151870   |     -      |     -     |   15.84  \n",
            "   2    |  1380   |   0.125575   |     -      |     -     |   15.86  \n",
            "   2    |  1400   |   0.113329   |     -      |     -     |   15.83  \n",
            "   2    |  1420   |   0.101707   |     -      |     -     |   15.82  \n",
            "   2    |  1440   |   0.052621   |     -      |     -     |   15.83  \n",
            "   2    |  1460   |   0.184422   |     -      |     -     |   15.85  \n",
            "   2    |  1480   |   0.129116   |     -      |     -     |   15.84  \n",
            "   2    |  1500   |   0.239954   |     -      |     -     |   15.83  \n",
            "   2    |  1520   |   0.181669   |     -      |     -     |   15.83  \n",
            "   2    |  1540   |   0.099778   |     -      |     -     |   15.84  \n",
            "   2    |  1560   |   0.212769   |     -      |     -     |   15.82  \n",
            "   2    |  1580   |   0.142338   |     -      |     -     |   15.82  \n",
            "   2    |  1600   |   0.108649   |     -      |     -     |   15.83  \n",
            "   2    |  1620   |   0.060383   |     -      |     -     |   15.83  \n",
            "   2    |  1640   |   0.124555   |     -      |     -     |   15.86  \n",
            "   2    |  1660   |   0.107654   |     -      |     -     |   15.84  \n",
            "   2    |  1680   |   0.133623   |     -      |     -     |   15.84  \n",
            "   2    |  1700   |   0.170401   |     -      |     -     |   15.83  \n",
            "   2    |  1720   |   0.088916   |     -      |     -     |   15.85  \n",
            "   2    |  1740   |   0.172880   |     -      |     -     |   15.85  \n",
            "   2    |  1760   |   0.137194   |     -      |     -     |   15.83  \n",
            "   2    |  1780   |   0.104981   |     -      |     -     |   15.85  \n",
            "   2    |  1800   |   0.085004   |     -      |     -     |   15.85  \n",
            "   2    |  1820   |   0.197185   |     -      |     -     |   15.84  \n",
            "   2    |  1840   |   0.095329   |     -      |     -     |   15.82  \n",
            "   2    |  1860   |   0.252901   |     -      |     -     |   15.84  \n",
            "   2    |  1880   |   0.146182   |     -      |     -     |   15.85  \n",
            "   2    |  1900   |   0.130166   |     -      |     -     |   15.84  \n",
            "   2    |  1920   |   0.111569   |     -      |     -     |   15.84  \n",
            "   2    |  1940   |   0.053848   |     -      |     -     |   15.84  \n",
            "   2    |  1960   |   0.131156   |     -      |     -     |   15.84  \n",
            "   2    |  1980   |   0.088170   |     -      |     -     |   15.87  \n",
            "   2    |  2000   |   0.174241   |     -      |     -     |   15.85  \n",
            "   2    |  2020   |   0.097915   |     -      |     -     |   15.83  \n",
            "   2    |  2040   |   0.129095   |     -      |     -     |   15.85  \n",
            "   2    |  2060   |   0.174397   |     -      |     -     |   15.84  \n",
            "   2    |  2080   |   0.106452   |     -      |     -     |   15.85  \n",
            "   2    |  2100   |   0.138095   |     -      |     -     |   15.85  \n",
            "   2    |  2120   |   0.091541   |     -      |     -     |   15.86  \n",
            "   2    |  2140   |   0.167112   |     -      |     -     |   15.87  \n",
            "   2    |  2160   |   0.175394   |     -      |     -     |   15.85  \n",
            "   2    |  2180   |   0.180198   |     -      |     -     |   15.82  \n",
            "   2    |  2200   |   0.120362   |     -      |     -     |   15.83  \n",
            "   2    |  2220   |   0.193929   |     -      |     -     |   15.83  \n",
            "   2    |  2240   |   0.114741   |     -      |     -     |   15.83  \n",
            "   2    |  2260   |   0.159946   |     -      |     -     |   15.83  \n",
            "   2    |  2280   |   0.132469   |     -      |     -     |   15.85  \n",
            "   2    |  2300   |   0.189559   |     -      |     -     |   15.84  \n",
            "   2    |  2320   |   0.100051   |     -      |     -     |   15.87  \n",
            "   2    |  2340   |   0.185949   |     -      |     -     |   15.89  \n",
            "   2    |  2360   |   0.097594   |     -      |     -     |   15.89  \n",
            "   2    |  2380   |   0.125503   |     -      |     -     |   15.88  \n",
            "   2    |  2400   |   0.110463   |     -      |     -     |   15.84  \n",
            "   2    |  2420   |   0.272303   |     -      |     -     |   15.83  \n",
            "   2    |  2440   |   0.098006   |     -      |     -     |   15.83  \n",
            "   2    |  2460   |   0.157582   |     -      |     -     |   15.83  \n",
            "   2    |  2480   |   0.098615   |     -      |     -     |   15.83  \n",
            "   2    |  2500   |   0.142525   |     -      |     -     |   15.84  \n",
            "   2    |  2520   |   0.115569   |     -      |     -     |   15.85  \n",
            "   2    |  2540   |   0.132652   |     -      |     -     |   15.83  \n",
            "   2    |  2560   |   0.228185   |     -      |     -     |   15.84  \n",
            "   2    |  2580   |   0.125115   |     -      |     -     |   15.85  \n",
            "   2    |  2600   |   0.150267   |     -      |     -     |   15.82  \n",
            "   2    |  2620   |   0.124677   |     -      |     -     |   15.85  \n",
            "   2    |  2640   |   0.104391   |     -      |     -     |   15.83  \n",
            "   2    |  2660   |   0.108277   |     -      |     -     |   15.85  \n",
            "   2    |  2680   |   0.154903   |     -      |     -     |   15.84  \n",
            "   2    |  2700   |   0.172826   |     -      |     -     |   15.84  \n",
            "   2    |  2720   |   0.086807   |     -      |     -     |   15.84  \n",
            "   2    |  2740   |   0.111207   |     -      |     -     |   15.85  \n",
            "   2    |  2760   |   0.230989   |     -      |     -     |   15.83  \n",
            "   2    |  2780   |   0.098314   |     -      |     -     |   15.83  \n",
            "   2    |  2800   |   0.161925   |     -      |     -     |   15.81  \n",
            "   2    |  2820   |   0.182898   |     -      |     -     |   15.84  \n",
            "   2    |  2840   |   0.106455   |     -      |     -     |   15.84  \n",
            "   2    |  2860   |   0.168325   |     -      |     -     |   15.85  \n",
            "   2    |  2880   |   0.122136   |     -      |     -     |   15.83  \n",
            "   2    |  2900   |   0.094932   |     -      |     -     |   15.83  \n",
            "   2    |  2920   |   0.113024   |     -      |     -     |   15.85  \n",
            "   2    |  2940   |   0.104981   |     -      |     -     |   15.87  \n",
            "   2    |  2960   |   0.107923   |     -      |     -     |   15.83  \n",
            "   2    |  2980   |   0.111719   |     -      |     -     |   15.83  \n",
            "   2    |  3000   |   0.236774   |     -      |     -     |   15.83  \n",
            "   2    |  3020   |   0.070594   |     -      |     -     |   15.83  \n",
            "   2    |  3040   |   0.030821   |     -      |     -     |   15.83  \n",
            "   2    |  3060   |   0.081729   |     -      |     -     |   15.83  \n",
            "   2    |  3080   |   0.039988   |     -      |     -     |   15.87  \n",
            "   2    |  3100   |   0.062420   |     -      |     -     |   15.85  \n",
            "   2    |  3120   |   0.202937   |     -      |     -     |   15.84  \n",
            "   2    |  3140   |   0.087266   |     -      |     -     |   15.84  \n",
            "   2    |  3160   |   0.067400   |     -      |     -     |   15.83  \n",
            "   2    |  3180   |   0.166786   |     -      |     -     |   15.83  \n",
            "   2    |  3200   |   0.145673   |     -      |     -     |   15.83  \n",
            "   2    |  3220   |   0.132138   |     -      |     -     |   15.83  \n",
            "   2    |  3240   |   0.143876   |     -      |     -     |   15.83  \n",
            "   2    |  3260   |   0.151811   |     -      |     -     |   15.84  \n",
            "   2    |  3280   |   0.082834   |     -      |     -     |   15.83  \n",
            "   2    |  3300   |   0.108930   |     -      |     -     |   15.83  \n",
            "   2    |  3320   |   0.114487   |     -      |     -     |   15.84  \n",
            "   2    |  3340   |   0.053714   |     -      |     -     |   15.84  \n",
            "   2    |  3360   |   0.129380   |     -      |     -     |   15.83  \n",
            "   2    |  3380   |   0.095479   |     -      |     -     |   15.82  \n",
            "   2    |  3400   |   0.168439   |     -      |     -     |   15.83  \n",
            "   2    |  3420   |   0.123970   |     -      |     -     |   15.84  \n",
            "   2    |  3440   |   0.109744   |     -      |     -     |   15.83  \n",
            "   2    |  3460   |   0.074927   |     -      |     -     |   15.83  \n",
            "   2    |  3480   |   0.190503   |     -      |     -     |   15.82  \n",
            "   2    |  3500   |   0.088407   |     -      |     -     |   15.83  \n",
            "   2    |  3520   |   0.175309   |     -      |     -     |   15.83  \n",
            "   2    |  3540   |   0.090297   |     -      |     -     |   15.83  \n",
            "   2    |  3560   |   0.095147   |     -      |     -     |   15.83  \n",
            "   2    |  3580   |   0.131940   |     -      |     -     |   15.83  \n",
            "   2    |  3600   |   0.125494   |     -      |     -     |   15.82  \n",
            "   2    |  3620   |   0.065875   |     -      |     -     |   15.83  \n",
            "   2    |  3640   |   0.166103   |     -      |     -     |   15.83  \n",
            "   2    |  3660   |   0.083509   |     -      |     -     |   15.82  \n",
            "   2    |  3680   |   0.078641   |     -      |     -     |   15.85  \n",
            "   2    |  3700   |   0.113094   |     -      |     -     |   15.85  \n",
            "   2    |  3720   |   0.097821   |     -      |     -     |   15.85  \n",
            "   2    |  3740   |   0.146240   |     -      |     -     |   15.83  \n",
            "   2    |  3760   |   0.130427   |     -      |     -     |   15.84  \n",
            "   2    |  3780   |   0.085967   |     -      |     -     |   15.84  \n",
            "   2    |  3800   |   0.122893   |     -      |     -     |   15.83  \n",
            "   2    |  3820   |   0.116472   |     -      |     -     |   15.83  \n",
            "   2    |  3840   |   0.182225   |     -      |     -     |   15.82  \n",
            "   2    |  3860   |   0.113699   |     -      |     -     |   15.85  \n",
            "   2    |  3880   |   0.200374   |     -      |     -     |   15.82  \n",
            "   2    |  3900   |   0.170768   |     -      |     -     |   15.84  \n",
            "   2    |  3920   |   0.059436   |     -      |     -     |   15.83  \n",
            "   2    |  3940   |   0.092027   |     -      |     -     |   15.85  \n",
            "   2    |  3950   |   0.037236   |     -      |     -     |   7.92   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.136213   |  0.199248  |   95.18   |  3245.78 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename = 'trained-twitter-roberta-base-sentiment-BiGRU.sav'\n",
        "pickle.dump(bert_classifier, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "Pi0d0Im5QxvH"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading the model (to avoid retraining in reruns)\n",
        "\n",
        "# import pickle\n",
        "# filename = 'trained-twitter-roberta-base-sentiment-BiGRU.sav'\n",
        "# f = open(filename, 'rb')\n",
        "# bert_classifier = pickle.load(f)"
      ],
      "metadata": {
        "id": "K-c0ZmRox8AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "Y84FNM8rlMuq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    #Get Precision and Recall over the test set\n",
        "    precision  = precision_score(y_true, y_pred, average='binary')\n",
        "    print(f'Precision: {precision*100:.2f}%')\n",
        "    recall = recall_score(y_true, y_pred, average='binary')\n",
        "    print(f'Recall: {recall*100:.2f}%')\n",
        "\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "    print('RoBERTa-Base-GRU: f1=%.3f ' % (f1))\n",
        "    # plot the precision-recall curves\n",
        "    baseline = len(y_test[y_test==1]) / len(y_test)\n",
        "    plt.plot([0, 1], [baseline, baseline], linestyle='--', label='Baseline')\n",
        "    plt.plot(recall, precision, marker='.', label='RoBERTa-Base-GRU')\n",
        "    # axis labels\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "BaP0b0jclMxY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted probabilities on the validation set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "MUH-ejXRlMz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "23d54ca2-1e7f-4d0e-f8ca-30c2e0e2c45c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9900\n",
            "Accuracy: 95.17%\n",
            "Precision: 94.90%\n",
            "Recall: 96.31%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa-Base-GRU: f1=0.956 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run `preprocessing_for_bert` on the test set\n",
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvvheq0qlM3d",
        "outputId": "7b464c21-a56c-437c-cd00-7f3ac8f1d9fc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENShAjsVlh0P",
        "outputId": "030f3a5b-572e-4ec4-bc0f-623e979c8b32"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no-negative tweets ratio  0.5602186663629634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "4nMBgpwvlh3D",
        "outputId": "3d78506e-a6aa-4879-9e43-07bf2ed48146"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9911\n",
            "Accuracy: 95.53%\n",
            "Precision: 95.59%\n",
            "Recall: 96.40%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa-Base-GRU: f1=0.960 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 1"
      ],
      "metadata": {
        "id": "UbxnxHjbrN7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.text.values\n",
        "y = df1.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "blHSYaz9rWGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "LHkD3v3DrWJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "PFgGW0g7rWMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "C15aScZ7rWQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "BV8qdxdorWW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "IWA5L_vXrWZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "vmU48X_wrWcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "q2J_5WbYrWe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 2"
      ],
      "metadata": {
        "id": "__XHLDr7sXzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df2.text.values\n",
        "y = df2.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "KL4fvGwtsfMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "UWZaHaRHsfMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "E9HVAST2sfMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "0DgNVgLYstSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "wsQvAkT3stSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "Rfb1hZkNstSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "k9kl_X-NstSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "CIYUYUDxstSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCx1zoZpsihF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 3"
      ],
      "metadata": {
        "id": "9P3Sl4TyskJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df3.text.values\n",
        "y = df3.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "NJaYb6qVsmgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "cDP6Aq1lsmgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "mGH5uKTBsmgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "Ohu1X0l4sv_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "ehKs_Hoasv_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "zWWGeZrEsv_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "NnilNQbIsv_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "MkPnUPx3sv_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mLr3anZ4siof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 4"
      ],
      "metadata": {
        "id": "EaM6piRas1XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df4.text.values\n",
        "y = df4.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "RbrKWSI3s91p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "fxx7ZTm1s91q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "T_vwMKHbs91r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "sCo5YrVZs2jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "HUp-hx4vs2jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "D8_J4ZsEs2jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "rSoWoSPZs2jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "qSqZKD1vs2jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QFJkGQigsits"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset 5"
      ],
      "metadata": {
        "id": "niWG8neUs4No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df5.text.values\n",
        "y = df5.sentiment.values\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "SQV4jrVms_90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing)[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing)"
      ],
      "metadata": {
        "id": "unFyjf8os_91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "OBSt28hOs_92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "CyEj0m9ms56o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "FJz8jcOQs56p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "hx1c_2xbs56q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"no-negative tweets ratio \", preds.sum()/len(preds))"
      ],
      "metadata": {
        "id": "2dz6at6Ys56q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Bert classifier for unseen test data\n",
        "evaluate_roc(probs, y_test)"
      ],
      "metadata": {
        "id": "UGl5A0X0s56q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}